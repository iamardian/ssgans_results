{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/ec-gan_digikalamag_p-10_w-1_t-20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grVysDqKl14Y",
        "outputId": "61a98d45-27c6-46f1-ba8f-10098cc90e3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "b26cb47c-afa2-45df-fe48-e3191af901e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/075_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/076_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 78 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "evaluation\n",
            "evaluation : 158 / 689 * 100 = 22.93178519593614 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "validation\n",
            "validate : 177 / 767 * 100 = 23.076923076923077 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/076_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/077_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/077_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/078_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/078_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/079_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/079_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/080_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/080_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/081_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/081_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/082_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/082_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/083_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/083_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/084_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/084_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/085_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/085_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/086_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/086_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/087_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/087_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/088_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/088_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/089_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/089_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/090_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/090_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/091_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "92     93  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/091_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/092_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "92     93  32.220610\n",
            "93     94  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/092_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/093_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "92     93  32.220610\n",
            "93     94  32.220610\n",
            "94     95  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/093_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/094_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "92     93  32.220610\n",
            "93     94  32.220610\n",
            "94     95  32.220610\n",
            "95     96  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/094_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/095_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "92     93  32.220610\n",
            "93     94  32.220610\n",
            "94     95  32.220610\n",
            "95     96  32.220610\n",
            "96     97  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "96     97  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/095_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/096_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "92     93  32.220610\n",
            "93     94  32.220610\n",
            "94     95  32.220610\n",
            "95     96  32.220610\n",
            "96     97  32.220610\n",
            "97     98  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "96     97  32.594524\n",
            "97     98  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/096_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/097_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "92     93  32.220610\n",
            "93     94  32.220610\n",
            "94     95  32.220610\n",
            "95     96  32.220610\n",
            "96     97  32.220610\n",
            "97     98  32.220610\n",
            "98     99  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "96     97  32.594524\n",
            "97     98  32.594524\n",
            "98     99  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/097_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/098_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     22.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     22.    Elapsed: 0:00:27.\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  56.458636\n",
            "1       2  74.600871\n",
            "2       3  90.130624\n",
            "3       4  92.743106\n",
            "4       5  88.969521\n",
            "5       6  93.178520\n",
            "6       7  95.500726\n",
            "7       8  95.210450\n",
            "8       9  95.500726\n",
            "9      10  95.355588\n",
            "10     11  96.226415\n",
            "11     12  96.226415\n",
            "12     13  96.226415\n",
            "13     14  96.371553\n",
            "14     15  96.516691\n",
            "15     16  96.516691\n",
            "16     17  96.516691\n",
            "17     18  96.661829\n",
            "18     19  96.661829\n",
            "19     20  94.775036\n",
            "20     21  96.226415\n",
            "21     22  95.065312\n",
            "22     23  92.162554\n",
            "23     24  91.872279\n",
            "24     25  53.555878\n",
            "25     26  53.701016\n",
            "26     27  51.959361\n",
            "27     28  51.959361\n",
            "28     29  39.912917\n",
            "29     30  42.525399\n",
            "30     31  35.558781\n",
            "31     32  22.931785\n",
            "32     33  22.931785\n",
            "33     34  17.706821\n",
            "34     35  22.931785\n",
            "35     36  32.220610\n",
            "36     37  32.220610\n",
            "37     38  52.830189\n",
            "38     39  51.959361\n",
            "39     40  51.523948\n",
            "40     41  53.410740\n",
            "41     42  21.480406\n",
            "42     43  32.220610\n",
            "43     44  32.220610\n",
            "44     45  32.220610\n",
            "45     46  32.220610\n",
            "46     47  32.220610\n",
            "47     48  32.220610\n",
            "48     49  32.220610\n",
            "49     50  32.220610\n",
            "50     51  32.220610\n",
            "51     52  32.220610\n",
            "52     53  32.220610\n",
            "53     54  32.220610\n",
            "54     55  32.220610\n",
            "55     56  32.220610\n",
            "56     57  32.220610\n",
            "57     58  32.220610\n",
            "58     59  32.220610\n",
            "59     60  32.220610\n",
            "60     61  32.220610\n",
            "61     62  32.220610\n",
            "62     63  32.220610\n",
            "63     64  22.931785\n",
            "64     65  32.220610\n",
            "65     66  32.220610\n",
            "66     67  32.220610\n",
            "67     68  32.220610\n",
            "68     69  32.220610\n",
            "69     70  32.220610\n",
            "70     71  32.220610\n",
            "71     72  32.220610\n",
            "72     73  32.220610\n",
            "73     74  32.220610\n",
            "74     75  32.220610\n",
            "75     76  32.220610\n",
            "76     77  32.220610\n",
            "77     78  22.931785\n",
            "78     79  32.220610\n",
            "79     80  32.220610\n",
            "80     81  32.220610\n",
            "81     82  32.220610\n",
            "82     83  32.220610\n",
            "83     84  32.220610\n",
            "84     85  32.220610\n",
            "85     86  32.220610\n",
            "86     87  32.220610\n",
            "87     88  32.220610\n",
            "88     89  32.220610\n",
            "89     90  32.220610\n",
            "90     91  32.220610\n",
            "91     92  32.220610\n",
            "92     93  32.220610\n",
            "93     94  32.220610\n",
            "94     95  32.220610\n",
            "95     96  32.220610\n",
            "96     97  32.220610\n",
            "97     98  32.220610\n",
            "98     99  32.220610\n",
            "99    100  32.220610\n",
            "evaluation\n",
            "evaluation : 222 / 689 * 100 = 32.22060957910015 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  55.932203\n",
            "1       2  73.011734\n",
            "2       3  85.136897\n",
            "3       4  90.221643\n",
            "4       5  85.788787\n",
            "5       6  88.396349\n",
            "6       7  90.221643\n",
            "7       8  91.395046\n",
            "8       9  90.091265\n",
            "9      10  89.308996\n",
            "10     11  89.960887\n",
            "11     12  89.569752\n",
            "12     13  89.700130\n",
            "13     14  89.439374\n",
            "14     15  90.221643\n",
            "15     16  90.221643\n",
            "16     17  89.960887\n",
            "17     18  90.482399\n",
            "18     19  91.134289\n",
            "19     20  86.440678\n",
            "20     21  90.482399\n",
            "21     22  88.135593\n",
            "22     23  86.440678\n",
            "23     24  84.745763\n",
            "24     25  50.586701\n",
            "25     26  50.717080\n",
            "26     27  47.979140\n",
            "27     28  48.370274\n",
            "28     29  35.462842\n",
            "29     30  39.895698\n",
            "30     31  33.507171\n",
            "31     32  23.076923\n",
            "32     33  23.076923\n",
            "33     34  18.904824\n",
            "34     35  23.076923\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  50.586701\n",
            "38     39  49.022164\n",
            "39     40  48.370274\n",
            "40     41  51.760104\n",
            "41     42  19.556714\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  23.076923\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  23.076923\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "96     97  32.594524\n",
            "97     98  32.594524\n",
            "98     99  32.594524\n",
            "99    100  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "best_model_accuracy : 91.39504563233378\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/098_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/099_digikalamag_0.1_0.01_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.1|0.01|0.2/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  90.375587\n",
            "test\n",
            "test : 770 / 852 * 100 = 90.3755868544601 \n",
            "       train data evaluation validation data evaluation\n",
            "0     [1, 56.45863570391872]    [1, 55.932203389830505]\n",
            "1     [2, 74.60087082728593]     [2, 73.01173402868318]\n",
            "2     [3, 90.13062409288824]     [3, 85.13689700130378]\n",
            "3     [4, 92.74310595065312]     [4, 90.22164276401564]\n",
            "4     [5, 88.96952104499275]     [5, 85.78878748370273]\n",
            "5     [6, 93.17851959361393]     [6, 88.39634941329857]\n",
            "6     [7, 95.50072568940493]     [7, 90.22164276401564]\n",
            "7     [8, 95.21044992743106]     [8, 91.39504563233378]\n",
            "8     [9, 95.50072568940493]     [9, 90.09126466753585]\n",
            "9      [10, 95.355587808418]    [10, 89.30899608865711]\n",
            "10   [11, 96.22641509433963]    [11, 89.96088657105607]\n",
            "11   [12, 96.22641509433963]    [12, 89.56975228161669]\n",
            "12   [13, 96.22641509433963]    [13, 89.70013037809647]\n",
            "13   [14, 96.37155297532655]     [14, 89.4393741851369]\n",
            "14    [15, 96.5166908563135]    [15, 90.22164276401564]\n",
            "15    [16, 96.5166908563135]    [16, 90.22164276401564]\n",
            "16    [17, 96.5166908563135]    [17, 89.96088657105607]\n",
            "17   [18, 96.66182873730044]    [18, 90.48239895697523]\n",
            "18   [19, 96.66182873730044]    [19, 91.13428943937419]\n",
            "19   [20, 94.77503628447025]     [20, 86.4406779661017]\n",
            "20   [21, 96.22641509433963]    [21, 90.48239895697523]\n",
            "21   [22, 95.06531204644412]    [22, 88.13559322033898]\n",
            "22   [23, 92.16255442670537]     [23, 86.4406779661017]\n",
            "23    [24, 91.8722786647315]     [24, 84.7457627118644]\n",
            "24   [25, 53.55587808417997]    [25, 50.58670143415907]\n",
            "25   [26, 53.70101596516691]   [26, 50.717079530638856]\n",
            "26  [27, 51.959361393323654]   [27, 47.979139504563236]\n",
            "27  [28, 51.959361393323654]    [28, 48.37027379400261]\n",
            "28  [29, 39.912917271407835]   [29, 35.462842242503264]\n",
            "29   [30, 42.52539912917271]   [30, 39.895697522816164]\n",
            "30   [31, 35.55878084179971]    [31, 33.50717079530639]\n",
            "31   [32, 22.93178519593614]   [32, 23.076923076923077]\n",
            "32   [33, 22.93178519593614]   [33, 23.076923076923077]\n",
            "33  [34, 17.706821480406386]   [34, 18.904823989569753]\n",
            "34   [35, 22.93178519593614]   [35, 23.076923076923077]\n",
            "35   [36, 32.22060957910015]    [36, 32.59452411994785]\n",
            "36   [37, 32.22060957910015]    [37, 32.59452411994785]\n",
            "37   [38, 52.83018867924528]    [38, 50.58670143415907]\n",
            "38  [39, 51.959361393323654]    [39, 49.02216427640156]\n",
            "39  [40, 51.523947750362844]    [40, 48.37027379400261]\n",
            "40   [41, 53.41074020319303]    [41, 51.76010430247718]\n",
            "41  [42, 21.480406386066765]   [42, 19.556714471968707]\n",
            "42   [43, 32.22060957910015]    [43, 32.59452411994785]\n",
            "43   [44, 32.22060957910015]    [44, 32.59452411994785]\n",
            "44   [45, 32.22060957910015]    [45, 32.59452411994785]\n",
            "45   [46, 32.22060957910015]    [46, 32.59452411994785]\n",
            "46   [47, 32.22060957910015]    [47, 32.59452411994785]\n",
            "47   [48, 32.22060957910015]    [48, 32.59452411994785]\n",
            "48   [49, 32.22060957910015]    [49, 32.59452411994785]\n",
            "49   [50, 32.22060957910015]    [50, 32.59452411994785]\n",
            "50   [51, 32.22060957910015]    [51, 32.59452411994785]\n",
            "51   [52, 32.22060957910015]    [52, 32.59452411994785]\n",
            "52   [53, 32.22060957910015]    [53, 32.59452411994785]\n",
            "53   [54, 32.22060957910015]    [54, 32.59452411994785]\n",
            "54   [55, 32.22060957910015]    [55, 32.59452411994785]\n",
            "55   [56, 32.22060957910015]    [56, 32.59452411994785]\n",
            "56   [57, 32.22060957910015]    [57, 32.59452411994785]\n",
            "57   [58, 32.22060957910015]    [58, 32.59452411994785]\n",
            "58   [59, 32.22060957910015]    [59, 32.59452411994785]\n",
            "59   [60, 32.22060957910015]    [60, 32.59452411994785]\n",
            "60   [61, 32.22060957910015]    [61, 32.59452411994785]\n",
            "61   [62, 32.22060957910015]    [62, 32.59452411994785]\n",
            "62   [63, 32.22060957910015]    [63, 32.59452411994785]\n",
            "63   [64, 22.93178519593614]   [64, 23.076923076923077]\n",
            "64   [65, 32.22060957910015]    [65, 32.59452411994785]\n",
            "65   [66, 32.22060957910015]    [66, 32.59452411994785]\n",
            "66   [67, 32.22060957910015]    [67, 32.59452411994785]\n",
            "67   [68, 32.22060957910015]    [68, 32.59452411994785]\n",
            "68   [69, 32.22060957910015]    [69, 32.59452411994785]\n",
            "69   [70, 32.22060957910015]    [70, 32.59452411994785]\n",
            "70   [71, 32.22060957910015]    [71, 32.59452411994785]\n",
            "71   [72, 32.22060957910015]    [72, 32.59452411994785]\n",
            "72   [73, 32.22060957910015]    [73, 32.59452411994785]\n",
            "73   [74, 32.22060957910015]    [74, 32.59452411994785]\n",
            "74   [75, 32.22060957910015]    [75, 32.59452411994785]\n",
            "75   [76, 32.22060957910015]    [76, 32.59452411994785]\n",
            "76   [77, 32.22060957910015]    [77, 32.59452411994785]\n",
            "77   [78, 22.93178519593614]   [78, 23.076923076923077]\n",
            "78   [79, 32.22060957910015]    [79, 32.59452411994785]\n",
            "79   [80, 32.22060957910015]    [80, 32.59452411994785]\n",
            "80   [81, 32.22060957910015]    [81, 32.59452411994785]\n",
            "81   [82, 32.22060957910015]    [82, 32.59452411994785]\n",
            "82   [83, 32.22060957910015]    [83, 32.59452411994785]\n",
            "83   [84, 32.22060957910015]    [84, 32.59452411994785]\n",
            "84   [85, 32.22060957910015]    [85, 32.59452411994785]\n",
            "85   [86, 32.22060957910015]    [86, 32.59452411994785]\n",
            "86   [87, 32.22060957910015]    [87, 32.59452411994785]\n",
            "87   [88, 32.22060957910015]    [88, 32.59452411994785]\n",
            "88   [89, 32.22060957910015]    [89, 32.59452411994785]\n",
            "89   [90, 32.22060957910015]    [90, 32.59452411994785]\n",
            "90   [91, 32.22060957910015]    [91, 32.59452411994785]\n",
            "91   [92, 32.22060957910015]    [92, 32.59452411994785]\n",
            "92   [93, 32.22060957910015]    [93, 32.59452411994785]\n",
            "93   [94, 32.22060957910015]    [94, 32.59452411994785]\n",
            "94   [95, 32.22060957910015]    [95, 32.59452411994785]\n",
            "95   [96, 32.22060957910015]    [96, 32.59452411994785]\n",
            "96   [97, 32.22060957910015]    [97, 32.59452411994785]\n",
            "97   [98, 32.22060957910015]    [98, 32.59452411994785]\n",
            "98   [99, 32.22060957910015]    [99, 32.59452411994785]\n",
            "99  [100, 32.22060957910015]   [100, 32.59452411994785]\n",
            "   test data evaluation\n",
            "0             90.375587\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d digikalamag -p 0.1 -w 0.01 -t 0.2 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}