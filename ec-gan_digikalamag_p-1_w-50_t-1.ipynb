{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/ec-gan_digikalamag_p-1_w-50_t-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2M91iHBzH0S",
        "outputId": "2f917272-ed9e-4551-c109-0f3824578eaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "27fb79df-59fc-46ef-8d82-898808a37bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "validation\n",
            "validate : 652 / 767 * 100 = 85.00651890482399 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/075_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/076_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 78 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "validation\n",
            "validate : 657 / 767 * 100 = 85.65840938722295 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/076_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/077_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "validation\n",
            "validate : 657 / 767 * 100 = 85.65840938722295 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/077_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/078_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "validation\n",
            "validate : 663 / 767 * 100 = 86.4406779661017 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/078_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/079_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "validation\n",
            "validate : 656 / 767 * 100 = 85.52803129074316 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/079_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/080_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "validation\n",
            "validate : 660 / 767 * 100 = 86.04954367666232 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/080_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/081_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "validation\n",
            "validate : 660 / 767 * 100 = 86.04954367666232 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/081_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/082_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "validation\n",
            "validate : 661 / 767 * 100 = 86.17992177314211 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/082_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/083_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "validation\n",
            "validate : 662 / 767 * 100 = 86.3102998696219 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/083_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/084_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "validation\n",
            "validate : 661 / 767 * 100 = 86.17992177314211 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/084_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/085_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "validation\n",
            "validate : 657 / 767 * 100 = 85.65840938722295 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/085_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/086_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "validation\n",
            "validate : 662 / 767 * 100 = 86.3102998696219 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/086_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/087_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "validation\n",
            "validate : 662 / 767 * 100 = 86.3102998696219 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/087_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/088_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "validation\n",
            "validate : 662 / 767 * 100 = 86.3102998696219 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/088_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/089_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "validation\n",
            "validate : 663 / 767 * 100 = 86.4406779661017 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/089_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/090_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "validation\n",
            "validate : 665 / 767 * 100 = 86.70143415906128 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/090_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/091_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "92     93   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "92     93  86.049544\n",
            "validation\n",
            "validate : 660 / 767 * 100 = 86.04954367666232 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/091_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/092_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "92     93   97.058824\n",
            "93     94   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "92     93  86.049544\n",
            "93     94  86.440678\n",
            "validation\n",
            "validate : 663 / 767 * 100 = 86.4406779661017 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/092_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/093_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "92     93   97.058824\n",
            "93     94   97.058824\n",
            "94     95   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "92     93  86.049544\n",
            "93     94  86.440678\n",
            "94     95  85.528031\n",
            "validation\n",
            "validate : 656 / 767 * 100 = 85.52803129074316 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/093_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/094_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "92     93   97.058824\n",
            "93     94   97.058824\n",
            "94     95   97.058824\n",
            "95     96   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "92     93  86.049544\n",
            "93     94  86.440678\n",
            "94     95  85.528031\n",
            "95     96  85.919166\n",
            "validation\n",
            "validate : 659 / 767 * 100 = 85.91916558018254 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/094_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/095_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "92     93   97.058824\n",
            "93     94   97.058824\n",
            "94     95   97.058824\n",
            "95     96   97.058824\n",
            "96     97   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "92     93  86.049544\n",
            "93     94  86.440678\n",
            "94     95  85.528031\n",
            "95     96  85.919166\n",
            "96     97  85.919166\n",
            "validation\n",
            "validate : 659 / 767 * 100 = 85.91916558018254 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/095_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/096_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "92     93   97.058824\n",
            "93     94   97.058824\n",
            "94     95   97.058824\n",
            "95     96   97.058824\n",
            "96     97   97.058824\n",
            "97     98   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "92     93  86.049544\n",
            "93     94  86.440678\n",
            "94     95  85.528031\n",
            "95     96  85.919166\n",
            "96     97  85.919166\n",
            "97     98  85.397653\n",
            "validation\n",
            "validate : 655 / 767 * 100 = 85.39765319426337 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/096_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/097_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "92     93   97.058824\n",
            "93     94   97.058824\n",
            "94     95   97.058824\n",
            "95     96   97.058824\n",
            "96     97   97.058824\n",
            "97     98   97.058824\n",
            "98     99   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "92     93  86.049544\n",
            "93     94  86.440678\n",
            "94     95  85.528031\n",
            "95     96  85.919166\n",
            "96     97  85.919166\n",
            "97     98  85.397653\n",
            "98     99  85.788787\n",
            "validation\n",
            "validate : 658 / 767 * 100 = 85.78878748370273 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/097_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/098_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   39.705882\n",
            "1       2   38.235294\n",
            "2       3   38.235294\n",
            "3       4   38.235294\n",
            "4       5   38.235294\n",
            "5       6   39.705882\n",
            "6       7   60.294118\n",
            "7       8   60.294118\n",
            "8       9   60.294118\n",
            "9      10   60.294118\n",
            "10     11   95.588235\n",
            "11     12   95.588235\n",
            "12     13   97.058824\n",
            "13     14   97.058824\n",
            "14     15   97.058824\n",
            "15     16   97.058824\n",
            "16     17   97.058824\n",
            "17     18   97.058824\n",
            "18     19   97.058824\n",
            "19     20   97.058824\n",
            "20     21   97.058824\n",
            "21     22   98.529412\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34   97.058824\n",
            "34     35   97.058824\n",
            "35     36   97.058824\n",
            "36     37   97.058824\n",
            "37     38   97.058824\n",
            "38     39   97.058824\n",
            "39     40   97.058824\n",
            "40     41   97.058824\n",
            "41     42   97.058824\n",
            "42     43   97.058824\n",
            "43     44   97.058824\n",
            "44     45   97.058824\n",
            "45     46   97.058824\n",
            "46     47   97.058824\n",
            "47     48   97.058824\n",
            "48     49   97.058824\n",
            "49     50   97.058824\n",
            "50     51   97.058824\n",
            "51     52   97.058824\n",
            "52     53   97.058824\n",
            "53     54   97.058824\n",
            "54     55   97.058824\n",
            "55     56   97.058824\n",
            "56     57   97.058824\n",
            "57     58   97.058824\n",
            "58     59   97.058824\n",
            "59     60   97.058824\n",
            "60     61   97.058824\n",
            "61     62   97.058824\n",
            "62     63   97.058824\n",
            "63     64   97.058824\n",
            "64     65   97.058824\n",
            "65     66   97.058824\n",
            "66     67   97.058824\n",
            "67     68   97.058824\n",
            "68     69   97.058824\n",
            "69     70   97.058824\n",
            "70     71   97.058824\n",
            "71     72   97.058824\n",
            "72     73   97.058824\n",
            "73     74   97.058824\n",
            "74     75   97.058824\n",
            "75     76   97.058824\n",
            "76     77   97.058824\n",
            "77     78   97.058824\n",
            "78     79   97.058824\n",
            "79     80   97.058824\n",
            "80     81   97.058824\n",
            "81     82   97.058824\n",
            "82     83   97.058824\n",
            "83     84   97.058824\n",
            "84     85   97.058824\n",
            "85     86   97.058824\n",
            "86     87   97.058824\n",
            "87     88   97.058824\n",
            "88     89   97.058824\n",
            "89     90   97.058824\n",
            "90     91   97.058824\n",
            "91     92   97.058824\n",
            "92     93   97.058824\n",
            "93     94   97.058824\n",
            "94     95   97.058824\n",
            "95     96   97.058824\n",
            "96     97   97.058824\n",
            "97     98   97.058824\n",
            "98     99   97.058824\n",
            "99    100   97.058824\n",
            "evaluation\n",
            "evaluation : 66 / 68 * 100 = 97.05882352941177 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  32.724902\n",
            "1       2  32.594524\n",
            "2       3  32.594524\n",
            "3       4  32.594524\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  50.195567\n",
            "7       8  50.065189\n",
            "8       9  50.586701\n",
            "9      10  51.238592\n",
            "10     11  79.009126\n",
            "11     12  78.487614\n",
            "12     13  83.572360\n",
            "13     14  84.093872\n",
            "14     15  85.136897\n",
            "15     16  86.310300\n",
            "16     17  85.788787\n",
            "17     18  86.962190\n",
            "18     19  86.701434\n",
            "19     20  83.050847\n",
            "20     21  84.354628\n",
            "21     22  86.310300\n",
            "22     23  86.440678\n",
            "23     24  86.440678\n",
            "24     25  86.831812\n",
            "25     26  87.222947\n",
            "26     27  86.179922\n",
            "27     28  86.831812\n",
            "28     29  86.179922\n",
            "29     30  86.310300\n",
            "30     31  86.440678\n",
            "31     32  87.222947\n",
            "32     33  87.483703\n",
            "33     34  86.962190\n",
            "34     35  84.093872\n",
            "35     36  87.483703\n",
            "36     37  86.571056\n",
            "37     38  84.224250\n",
            "38     39  85.006519\n",
            "39     40  84.354628\n",
            "40     41  84.745763\n",
            "41     42  85.267275\n",
            "42     43  85.267275\n",
            "43     44  85.397653\n",
            "44     45  85.528031\n",
            "45     46  85.528031\n",
            "46     47  85.528031\n",
            "47     48  85.788787\n",
            "48     49  85.919166\n",
            "49     50  86.049544\n",
            "50     51  86.049544\n",
            "51     52  85.919166\n",
            "52     53  86.049544\n",
            "53     54  85.658409\n",
            "54     55  85.528031\n",
            "55     56  86.179922\n",
            "56     57  85.788787\n",
            "57     58  85.397653\n",
            "58     59  85.528031\n",
            "59     60  85.006519\n",
            "60     61  85.136897\n",
            "61     62  85.658409\n",
            "62     63  85.528031\n",
            "63     64  85.919166\n",
            "64     65  85.397653\n",
            "65     66  85.788787\n",
            "66     67  86.049544\n",
            "67     68  85.267275\n",
            "68     69  85.267275\n",
            "69     70  85.267275\n",
            "70     71  85.136897\n",
            "71     72  84.745763\n",
            "72     73  85.267275\n",
            "73     74  85.528031\n",
            "74     75  85.528031\n",
            "75     76  85.136897\n",
            "76     77  85.006519\n",
            "77     78  85.658409\n",
            "78     79  85.658409\n",
            "79     80  86.440678\n",
            "80     81  85.528031\n",
            "81     82  86.049544\n",
            "82     83  86.049544\n",
            "83     84  86.179922\n",
            "84     85  86.310300\n",
            "85     86  86.179922\n",
            "86     87  85.658409\n",
            "87     88  86.310300\n",
            "88     89  86.310300\n",
            "89     90  86.310300\n",
            "90     91  86.440678\n",
            "91     92  86.701434\n",
            "92     93  86.049544\n",
            "93     94  86.440678\n",
            "94     95  85.528031\n",
            "95     96  85.919166\n",
            "96     97  85.919166\n",
            "97     98  85.397653\n",
            "98     99  85.788787\n",
            "99    100  86.831812\n",
            "validation\n",
            "validate : 666 / 767 * 100 = 86.83181225554108 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "best_model_accuracy : 87.48370273794002\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/098_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/099_digikalamag_0.01_0.5_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.01|0.5|0.01/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  85.798122\n",
            "test\n",
            "test : 731 / 852 * 100 = 85.7981220657277 \n",
            "       train data evaluation validation data evaluation\n",
            "0    [1, 39.705882352941174]    [1, 32.724902216427644]\n",
            "1     [2, 38.23529411764706]     [2, 32.59452411994785]\n",
            "2     [3, 38.23529411764706]     [3, 32.59452411994785]\n",
            "3     [4, 38.23529411764706]     [4, 32.59452411994785]\n",
            "4     [5, 38.23529411764706]     [5, 32.59452411994785]\n",
            "5    [6, 39.705882352941174]     [6, 32.59452411994785]\n",
            "6     [7, 60.29411764705882]     [7, 50.19556714471969]\n",
            "7     [8, 60.29411764705882]      [8, 50.0651890482399]\n",
            "8     [9, 60.29411764705882]     [9, 50.58670143415907]\n",
            "9    [10, 60.29411764705882]    [10, 51.23859191655802]\n",
            "10   [11, 95.58823529411765]    [11, 79.00912646675359]\n",
            "11   [12, 95.58823529411765]    [12, 78.48761408083442]\n",
            "12   [13, 97.05882352941177]    [13, 83.57235984354628]\n",
            "13   [14, 97.05882352941177]    [14, 84.09387222946545]\n",
            "14   [15, 97.05882352941177]    [15, 85.13689700130378]\n",
            "15   [16, 97.05882352941177]     [16, 86.3102998696219]\n",
            "16   [17, 97.05882352941177]    [17, 85.78878748370273]\n",
            "17   [18, 97.05882352941177]    [18, 86.96219035202087]\n",
            "18   [19, 97.05882352941177]    [19, 86.70143415906128]\n",
            "19   [20, 97.05882352941177]    [20, 83.05084745762711]\n",
            "20   [21, 97.05882352941177]    [21, 84.35462842242504]\n",
            "21   [22, 98.52941176470588]     [22, 86.3102998696219]\n",
            "22               [23, 100.0]     [23, 86.4406779661017]\n",
            "23               [24, 100.0]     [24, 86.4406779661017]\n",
            "24               [25, 100.0]    [25, 86.83181225554108]\n",
            "25               [26, 100.0]    [26, 87.22294654498045]\n",
            "26               [27, 100.0]    [27, 86.17992177314211]\n",
            "27               [28, 100.0]    [28, 86.83181225554108]\n",
            "28               [29, 100.0]    [29, 86.17992177314211]\n",
            "29               [30, 100.0]     [30, 86.3102998696219]\n",
            "30               [31, 100.0]     [31, 86.4406779661017]\n",
            "31               [32, 100.0]    [32, 87.22294654498045]\n",
            "32               [33, 100.0]    [33, 87.48370273794002]\n",
            "33   [34, 97.05882352941177]    [34, 86.96219035202087]\n",
            "34   [35, 97.05882352941177]    [35, 84.09387222946545]\n",
            "35   [36, 97.05882352941177]    [36, 87.48370273794002]\n",
            "36   [37, 97.05882352941177]    [37, 86.57105606258149]\n",
            "37   [38, 97.05882352941177]    [38, 84.22425032594523]\n",
            "38   [39, 97.05882352941177]    [39, 85.00651890482399]\n",
            "39   [40, 97.05882352941177]    [40, 84.35462842242504]\n",
            "40   [41, 97.05882352941177]     [41, 84.7457627118644]\n",
            "41   [42, 97.05882352941177]    [42, 85.26727509778357]\n",
            "42   [43, 97.05882352941177]    [43, 85.26727509778357]\n",
            "43   [44, 97.05882352941177]    [44, 85.39765319426337]\n",
            "44   [45, 97.05882352941177]    [45, 85.52803129074316]\n",
            "45   [46, 97.05882352941177]    [46, 85.52803129074316]\n",
            "46   [47, 97.05882352941177]    [47, 85.52803129074316]\n",
            "47   [48, 97.05882352941177]    [48, 85.78878748370273]\n",
            "48   [49, 97.05882352941177]    [49, 85.91916558018254]\n",
            "49   [50, 97.05882352941177]    [50, 86.04954367666232]\n",
            "50   [51, 97.05882352941177]    [51, 86.04954367666232]\n",
            "51   [52, 97.05882352941177]    [52, 85.91916558018254]\n",
            "52   [53, 97.05882352941177]    [53, 86.04954367666232]\n",
            "53   [54, 97.05882352941177]    [54, 85.65840938722295]\n",
            "54   [55, 97.05882352941177]    [55, 85.52803129074316]\n",
            "55   [56, 97.05882352941177]    [56, 86.17992177314211]\n",
            "56   [57, 97.05882352941177]    [57, 85.78878748370273]\n",
            "57   [58, 97.05882352941177]    [58, 85.39765319426337]\n",
            "58   [59, 97.05882352941177]    [59, 85.52803129074316]\n",
            "59   [60, 97.05882352941177]    [60, 85.00651890482399]\n",
            "60   [61, 97.05882352941177]    [61, 85.13689700130378]\n",
            "61   [62, 97.05882352941177]    [62, 85.65840938722295]\n",
            "62   [63, 97.05882352941177]    [63, 85.52803129074316]\n",
            "63   [64, 97.05882352941177]    [64, 85.91916558018254]\n",
            "64   [65, 97.05882352941177]    [65, 85.39765319426337]\n",
            "65   [66, 97.05882352941177]    [66, 85.78878748370273]\n",
            "66   [67, 97.05882352941177]    [67, 86.04954367666232]\n",
            "67   [68, 97.05882352941177]    [68, 85.26727509778357]\n",
            "68   [69, 97.05882352941177]    [69, 85.26727509778357]\n",
            "69   [70, 97.05882352941177]    [70, 85.26727509778357]\n",
            "70   [71, 97.05882352941177]    [71, 85.13689700130378]\n",
            "71   [72, 97.05882352941177]     [72, 84.7457627118644]\n",
            "72   [73, 97.05882352941177]    [73, 85.26727509778357]\n",
            "73   [74, 97.05882352941177]    [74, 85.52803129074316]\n",
            "74   [75, 97.05882352941177]    [75, 85.52803129074316]\n",
            "75   [76, 97.05882352941177]    [76, 85.13689700130378]\n",
            "76   [77, 97.05882352941177]    [77, 85.00651890482399]\n",
            "77   [78, 97.05882352941177]    [78, 85.65840938722295]\n",
            "78   [79, 97.05882352941177]    [79, 85.65840938722295]\n",
            "79   [80, 97.05882352941177]     [80, 86.4406779661017]\n",
            "80   [81, 97.05882352941177]    [81, 85.52803129074316]\n",
            "81   [82, 97.05882352941177]    [82, 86.04954367666232]\n",
            "82   [83, 97.05882352941177]    [83, 86.04954367666232]\n",
            "83   [84, 97.05882352941177]    [84, 86.17992177314211]\n",
            "84   [85, 97.05882352941177]     [85, 86.3102998696219]\n",
            "85   [86, 97.05882352941177]    [86, 86.17992177314211]\n",
            "86   [87, 97.05882352941177]    [87, 85.65840938722295]\n",
            "87   [88, 97.05882352941177]     [88, 86.3102998696219]\n",
            "88   [89, 97.05882352941177]     [89, 86.3102998696219]\n",
            "89   [90, 97.05882352941177]     [90, 86.3102998696219]\n",
            "90   [91, 97.05882352941177]     [91, 86.4406779661017]\n",
            "91   [92, 97.05882352941177]    [92, 86.70143415906128]\n",
            "92   [93, 97.05882352941177]    [93, 86.04954367666232]\n",
            "93   [94, 97.05882352941177]     [94, 86.4406779661017]\n",
            "94   [95, 97.05882352941177]    [95, 85.52803129074316]\n",
            "95   [96, 97.05882352941177]    [96, 85.91916558018254]\n",
            "96   [97, 97.05882352941177]    [97, 85.91916558018254]\n",
            "97   [98, 97.05882352941177]    [98, 85.39765319426337]\n",
            "98   [99, 97.05882352941177]    [99, 85.78878748370273]\n",
            "99  [100, 97.05882352941177]   [100, 86.83181225554108]\n",
            "   test data evaluation\n",
            "0             85.798122\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d digikalamag -p 0.01 -w 0.5 -t 0.01 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}