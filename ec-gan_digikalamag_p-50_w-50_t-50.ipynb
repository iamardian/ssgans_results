{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/ec-gan_digikalamag_p-50_w-50_t-50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqYvNzz3DZ5",
        "outputId": "d917ebd4-da95-4dff-ba2f-c23875cc407e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "146dcdf6-9cfe-4711-830a-75a03a470ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/076_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/077_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:47.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/077_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/078_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/078_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/079_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:47.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/079_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/080_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/080_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/081_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:47.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/081_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/082_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:47.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/082_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/083_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/083_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/084_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/084_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/085_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/085_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/086_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:47.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/086_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/087_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:47.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/087_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/088_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/088_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/089_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/089_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/090_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/090_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/091_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:01:59.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "92     93  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/091_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/092_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:01:59.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "92     93  32.540603\n",
            "93     94  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/092_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/093_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:47.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "92     93  32.540603\n",
            "93     94  32.540603\n",
            "94     95  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/093_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/094_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "92     93  32.540603\n",
            "93     94  32.540603\n",
            "94     95  32.540603\n",
            "95     96  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/094_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/095_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "92     93  32.540603\n",
            "93     94  32.540603\n",
            "94     95  32.540603\n",
            "95     96  32.540603\n",
            "96     97  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "96     97  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/095_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/096_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "92     93  32.540603\n",
            "93     94  32.540603\n",
            "94     95  32.540603\n",
            "95     96  32.540603\n",
            "96     97  32.540603\n",
            "97     98  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "96     97  32.594524\n",
            "97     98  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/096_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/097_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:40.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:46.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "92     93  32.540603\n",
            "93     94  32.540603\n",
            "94     95  32.540603\n",
            "95     96  32.540603\n",
            "96     97  32.540603\n",
            "97     98  32.540603\n",
            "98     99  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "96     97  32.594524\n",
            "97     98  32.594524\n",
            "98     99  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/097_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/098_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of    108.    Elapsed: 0:00:13.\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    108.    Elapsed: 0:01:07.\n",
            "  Batch    60  of    108.    Elapsed: 0:01:20.\n",
            "  Batch    70  of    108.    Elapsed: 0:01:33.\n",
            "  Batch    80  of    108.    Elapsed: 0:01:47.\n",
            "  Batch    90  of    108.    Elapsed: 0:02:00.\n",
            "  Batch   100  of    108.    Elapsed: 0:02:13.\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  88.921114\n",
            "1       2  88.921114\n",
            "2       3  70.852668\n",
            "3       4  50.290023\n",
            "4       5  32.569606\n",
            "5       6  32.598608\n",
            "6       7  32.540603\n",
            "7       8  32.540603\n",
            "8       9  32.540603\n",
            "9      10  32.540603\n",
            "10     11  32.540603\n",
            "11     12  32.540603\n",
            "12     13  22.795824\n",
            "13     14  32.540603\n",
            "14     15  32.540603\n",
            "15     16  32.540603\n",
            "16     17  32.540603\n",
            "17     18  32.540603\n",
            "18     19  32.540603\n",
            "19     20  32.540603\n",
            "20     21  32.540603\n",
            "21     22  32.540603\n",
            "22     23  32.540603\n",
            "23     24  32.540603\n",
            "24     25  32.540603\n",
            "25     26  32.540603\n",
            "26     27  32.540603\n",
            "27     28  32.540603\n",
            "28     29  32.540603\n",
            "29     30  32.540603\n",
            "30     31  32.540603\n",
            "31     32  32.540603\n",
            "32     33  32.540603\n",
            "33     34  32.540603\n",
            "34     35  32.540603\n",
            "35     36  32.540603\n",
            "36     37  32.540603\n",
            "37     38  32.540603\n",
            "38     39  32.540603\n",
            "39     40  32.540603\n",
            "40     41  32.540603\n",
            "41     42  32.540603\n",
            "42     43  32.540603\n",
            "43     44  32.540603\n",
            "44     45  32.540603\n",
            "45     46  32.540603\n",
            "46     47  32.540603\n",
            "47     48  32.540603\n",
            "48     49  32.540603\n",
            "49     50  32.540603\n",
            "50     51  32.540603\n",
            "51     52  32.540603\n",
            "52     53  32.540603\n",
            "53     54  32.540603\n",
            "54     55  32.540603\n",
            "55     56  32.540603\n",
            "56     57  32.540603\n",
            "57     58  32.540603\n",
            "58     59  32.540603\n",
            "59     60  32.540603\n",
            "60     61  32.540603\n",
            "61     62  32.540603\n",
            "62     63  32.540603\n",
            "63     64  32.540603\n",
            "64     65  32.540603\n",
            "65     66  32.540603\n",
            "66     67  32.540603\n",
            "67     68  32.540603\n",
            "68     69  32.540603\n",
            "69     70  32.540603\n",
            "70     71  32.540603\n",
            "71     72  32.540603\n",
            "72     73  32.540603\n",
            "73     74  32.540603\n",
            "74     75  32.540603\n",
            "75     76  32.540603\n",
            "76     77  32.540603\n",
            "77     78  32.540603\n",
            "78     79  32.540603\n",
            "79     80  32.540603\n",
            "80     81  32.540603\n",
            "81     82  32.540603\n",
            "82     83  32.540603\n",
            "83     84  32.540603\n",
            "84     85  32.540603\n",
            "85     86  32.540603\n",
            "86     87  32.540603\n",
            "87     88  32.540603\n",
            "88     89  32.540603\n",
            "89     90  32.540603\n",
            "90     91  32.540603\n",
            "91     92  32.540603\n",
            "92     93  32.540603\n",
            "93     94  32.540603\n",
            "94     95  32.540603\n",
            "95     96  32.540603\n",
            "96     97  32.540603\n",
            "97     98  32.540603\n",
            "98     99  32.540603\n",
            "99    100  32.540603\n",
            "evaluation\n",
            "evaluation : 1122 / 3448 * 100 = 32.540603248259856 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  88.917862\n",
            "1       2  88.526728\n",
            "2       3  71.056063\n",
            "3       4  49.674055\n",
            "4       5  32.594524\n",
            "5       6  32.594524\n",
            "6       7  32.594524\n",
            "7       8  32.594524\n",
            "8       9  32.594524\n",
            "9      10  32.594524\n",
            "10     11  32.594524\n",
            "11     12  32.594524\n",
            "12     13  23.076923\n",
            "13     14  32.594524\n",
            "14     15  32.594524\n",
            "15     16  32.594524\n",
            "16     17  32.594524\n",
            "17     18  32.594524\n",
            "18     19  32.594524\n",
            "19     20  32.594524\n",
            "20     21  32.594524\n",
            "21     22  32.594524\n",
            "22     23  32.594524\n",
            "23     24  32.594524\n",
            "24     25  32.594524\n",
            "25     26  32.594524\n",
            "26     27  32.594524\n",
            "27     28  32.594524\n",
            "28     29  32.594524\n",
            "29     30  32.594524\n",
            "30     31  32.594524\n",
            "31     32  32.594524\n",
            "32     33  32.594524\n",
            "33     34  32.594524\n",
            "34     35  32.594524\n",
            "35     36  32.594524\n",
            "36     37  32.594524\n",
            "37     38  32.594524\n",
            "38     39  32.594524\n",
            "39     40  32.594524\n",
            "40     41  32.594524\n",
            "41     42  32.594524\n",
            "42     43  32.594524\n",
            "43     44  32.594524\n",
            "44     45  32.594524\n",
            "45     46  32.594524\n",
            "46     47  32.594524\n",
            "47     48  32.594524\n",
            "48     49  32.594524\n",
            "49     50  32.594524\n",
            "50     51  32.594524\n",
            "51     52  32.594524\n",
            "52     53  32.594524\n",
            "53     54  32.594524\n",
            "54     55  32.594524\n",
            "55     56  32.594524\n",
            "56     57  32.594524\n",
            "57     58  32.594524\n",
            "58     59  32.594524\n",
            "59     60  32.594524\n",
            "60     61  32.594524\n",
            "61     62  32.594524\n",
            "62     63  32.594524\n",
            "63     64  32.594524\n",
            "64     65  32.594524\n",
            "65     66  32.594524\n",
            "66     67  32.594524\n",
            "67     68  32.594524\n",
            "68     69  32.594524\n",
            "69     70  32.594524\n",
            "70     71  32.594524\n",
            "71     72  32.594524\n",
            "72     73  32.594524\n",
            "73     74  32.594524\n",
            "74     75  32.594524\n",
            "75     76  32.594524\n",
            "76     77  32.594524\n",
            "77     78  32.594524\n",
            "78     79  32.594524\n",
            "79     80  32.594524\n",
            "80     81  32.594524\n",
            "81     82  32.594524\n",
            "82     83  32.594524\n",
            "83     84  32.594524\n",
            "84     85  32.594524\n",
            "85     86  32.594524\n",
            "86     87  32.594524\n",
            "87     88  32.594524\n",
            "88     89  32.594524\n",
            "89     90  32.594524\n",
            "90     91  32.594524\n",
            "91     92  32.594524\n",
            "92     93  32.594524\n",
            "93     94  32.594524\n",
            "94     95  32.594524\n",
            "95     96  32.594524\n",
            "96     97  32.594524\n",
            "97     98  32.594524\n",
            "98     99  32.594524\n",
            "99    100  32.594524\n",
            "validation\n",
            "validate : 250 / 767 * 100 = 32.59452411994785 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "best_model_accuracy : 88.91786179921773\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/098_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/099_digikalamag_0.5_0.5_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|digikalamag|0.5|0.5|0.5/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  86.619718\n",
            "test\n",
            "test : 738 / 852 * 100 = 86.61971830985915 \n",
            "        train data evaluation validation data evaluation\n",
            "0      [1, 88.92111368909514]     [1, 88.91786179921773]\n",
            "1      [2, 88.92111368909514]     [2, 88.52672750977835]\n",
            "2      [3, 70.85266821345708]     [3, 71.05606258148632]\n",
            "3      [4, 50.29002320185615]     [4, 49.67405475880052]\n",
            "4     [5, 32.569605568445475]     [5, 32.59452411994785]\n",
            "..                        ...                        ...\n",
            "95   [96, 32.540603248259856]    [96, 32.59452411994785]\n",
            "96   [97, 32.540603248259856]    [97, 32.59452411994785]\n",
            "97   [98, 32.540603248259856]    [98, 32.59452411994785]\n",
            "98   [99, 32.540603248259856]    [99, 32.59452411994785]\n",
            "99  [100, 32.540603248259856]   [100, 32.59452411994785]\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "   test data evaluation\n",
            "0             86.619718\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d digikalamag -p 0.5 -w 0.5 -t 0.5 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}