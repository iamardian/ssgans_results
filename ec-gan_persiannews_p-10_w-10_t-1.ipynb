{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/ec-gan_persiannews_p-10_w-10_t-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl6ZOMZwlVBL",
        "outputId": "5e448657-c78f-4bca-d1bb-00fbd8e51f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVzghhQznhMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a14f016-b9aa-441c-eb36-2c1c6ad83c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "evaluation\n",
            "evaluation : 151 / 1331 * 100 = 11.344853493613824 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "validation\n",
            "validate : 178 / 1480 * 100 = 12.027027027027028 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/076_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/077_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/077_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/078_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/078_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/079_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/079_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/080_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "evaluation\n",
            "evaluation : 335 / 1331 * 100 = 25.169045830202858 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "validation\n",
            "validate : 329 / 1480 * 100 = 22.22972972972973 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/080_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/081_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/081_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/082_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/082_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/083_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/083_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/084_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:54.\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/084_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/085_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/085_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/086_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/086_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/087_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/087_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/088_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/088_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/089_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/089_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/090_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/090_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/091_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/091_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/092_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/092_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/093_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "evaluation\n",
            "evaluation : 209 / 1331 * 100 = 15.702479338842975 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "validation\n",
            "validate : 219 / 1480 * 100 = 14.797297297297296 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/093_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/094_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/094_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/095_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "96     97  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "96     97  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/095_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/096_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "96     97  16.153268\n",
            "97     98  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "96     97  15.540541\n",
            "97     98  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/096_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/097_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "96     97  16.153268\n",
            "97     98  16.153268\n",
            "98     99  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "96     97  15.540541\n",
            "97     98  15.540541\n",
            "98     99  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/097_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/098_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  64.312547\n",
            "1       2  87.152517\n",
            "2       3  95.041322\n",
            "3       4  92.561983\n",
            "4       5  97.295267\n",
            "5       6  90.909091\n",
            "6       7  97.670924\n",
            "7       8  98.196844\n",
            "8       9  96.769346\n",
            "9      10  96.919609\n",
            "10     11  94.891059\n",
            "11     12  94.966191\n",
            "12     13  96.243426\n",
            "13     14  87.603306\n",
            "14     15  81.667919\n",
            "15     16  49.812171\n",
            "16     17  82.794891\n",
            "17     18  74.455297\n",
            "18     19  71.224643\n",
            "19     20  80.691210\n",
            "20     21  91.510143\n",
            "21     22  94.064613\n",
            "22     23  94.891059\n",
            "23     24  94.365139\n",
            "24     25  96.468820\n",
            "25     26  96.318557\n",
            "26     27  96.543952\n",
            "27     28  96.619083\n",
            "28     29  96.919609\n",
            "29     30  94.440270\n",
            "30     31  93.613824\n",
            "31     32  84.898573\n",
            "32     33  78.287002\n",
            "33     34  86.626597\n",
            "34     35  53.268219\n",
            "35     36  41.397446\n",
            "36     37  41.397446\n",
            "37     38  24.417731\n",
            "38     39  25.018783\n",
            "39     40  24.567994\n",
            "40     41  24.943651\n",
            "41     42  25.619835\n",
            "42     43  25.845229\n",
            "43     44  26.070624\n",
            "44     45  26.145755\n",
            "45     46  40.420736\n",
            "46     47  38.617581\n",
            "47     48  36.363636\n",
            "48     49  28.549962\n",
            "49     50  13.523666\n",
            "50     51  12.697220\n",
            "51     52  12.697220\n",
            "52     53  12.697220\n",
            "53     54  12.697220\n",
            "54     55  12.697220\n",
            "55     56  12.697220\n",
            "56     57  12.697220\n",
            "57     58  15.702479\n",
            "58     59  13.523666\n",
            "59     60  15.702479\n",
            "60     61  15.702479\n",
            "61     62  12.697220\n",
            "62     63  12.697220\n",
            "63     64  12.697220\n",
            "64     65  12.697220\n",
            "65     66  12.697220\n",
            "66     67  12.697220\n",
            "67     68  13.523666\n",
            "68     69  16.153268\n",
            "69     70  15.702479\n",
            "70     71  12.697220\n",
            "71     72  15.702479\n",
            "72     73  12.697220\n",
            "73     74  16.153268\n",
            "74     75  13.974455\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  11.344853\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  25.169046\n",
            "82     83  15.702479\n",
            "83     84  15.702479\n",
            "84     85  15.702479\n",
            "85     86  15.702479\n",
            "86     87  16.153268\n",
            "87     88  16.153268\n",
            "88     89  16.153268\n",
            "89     90  16.153268\n",
            "90     91  13.523666\n",
            "91     92  13.974455\n",
            "92     93  16.153268\n",
            "93     94  16.153268\n",
            "94     95  15.702479\n",
            "95     96  13.974455\n",
            "96     97  16.153268\n",
            "97     98  16.153268\n",
            "98     99  16.153268\n",
            "99    100  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  60.608108\n",
            "1       2  83.243243\n",
            "2       3  90.135135\n",
            "3       4  88.513514\n",
            "4       5  92.635135\n",
            "5       6  86.554054\n",
            "6       7  92.635135\n",
            "7       8  93.040541\n",
            "8       9  91.824324\n",
            "9      10  91.756757\n",
            "10     11  88.918919\n",
            "11     12  89.527027\n",
            "12     13  90.810811\n",
            "13     14  81.554054\n",
            "14     15  79.121622\n",
            "15     16  44.527027\n",
            "16     17  78.648649\n",
            "17     18  68.783784\n",
            "18     19  67.500000\n",
            "19     20  76.283784\n",
            "20     21  83.783784\n",
            "21     22  88.040541\n",
            "22     23  89.054054\n",
            "23     24  90.067568\n",
            "24     25  90.878378\n",
            "25     26  90.675676\n",
            "26     27  91.216216\n",
            "27     28  91.283784\n",
            "28     29  90.810811\n",
            "29     30  87.432432\n",
            "30     31  87.770270\n",
            "31     32  78.243243\n",
            "32     33  70.202703\n",
            "33     34  80.743243\n",
            "34     35  46.824324\n",
            "35     36  38.581081\n",
            "36     37  38.378378\n",
            "37     38  22.094595\n",
            "38     39  23.108108\n",
            "39     40  22.229730\n",
            "40     41  22.364865\n",
            "41     42  23.310811\n",
            "42     43  24.054054\n",
            "43     44  24.459459\n",
            "44     45  25.270270\n",
            "45     46  36.891892\n",
            "46     47  34.729730\n",
            "47     48  33.716216\n",
            "48     49  26.081081\n",
            "49     50  13.243243\n",
            "50     51  12.702703\n",
            "51     52  12.702703\n",
            "52     53  12.702703\n",
            "53     54  12.702703\n",
            "54     55  12.702703\n",
            "55     56  12.702703\n",
            "56     57  12.702703\n",
            "57     58  14.797297\n",
            "58     59  13.243243\n",
            "59     60  14.797297\n",
            "60     61  14.797297\n",
            "61     62  12.702703\n",
            "62     63  12.702703\n",
            "63     64  12.702703\n",
            "64     65  12.702703\n",
            "65     66  12.702703\n",
            "66     67  12.702703\n",
            "67     68  13.243243\n",
            "68     69  15.540541\n",
            "69     70  14.797297\n",
            "70     71  12.702703\n",
            "71     72  14.797297\n",
            "72     73  12.702703\n",
            "73     74  15.540541\n",
            "74     75  13.783784\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  12.027027\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  22.229730\n",
            "82     83  14.797297\n",
            "83     84  14.797297\n",
            "84     85  14.797297\n",
            "85     86  14.797297\n",
            "86     87  15.540541\n",
            "87     88  15.540541\n",
            "88     89  15.540541\n",
            "89     90  15.540541\n",
            "90     91  13.243243\n",
            "91     92  13.783784\n",
            "92     93  15.540541\n",
            "93     94  15.540541\n",
            "94     95  14.797297\n",
            "95     96  13.783784\n",
            "96     97  15.540541\n",
            "97     98  15.540541\n",
            "98     99  15.540541\n",
            "99    100  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "best_model_accuracy : 93.04054054054053\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/098_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/099_persiannews_0.1_0.1_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.01/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  94.221411\n",
            "test\n",
            "test : 1549 / 1644 * 100 = 94.2214111922141 \n",
            "       train data evaluation validation data evaluation\n",
            "0     [1, 64.31254695717506]    [1, 60.608108108108105]\n",
            "1     [2, 87.15251690458302]     [2, 83.24324324324324]\n",
            "2      [3, 95.0413223140496]     [3, 90.13513513513513]\n",
            "3     [4, 92.56198347107438]     [4, 88.51351351351352]\n",
            "4     [5, 97.29526671675433]     [5, 92.63513513513514]\n",
            "5      [6, 90.9090909090909]     [6, 86.55405405405405]\n",
            "6      [7, 97.6709241172051]     [7, 92.63513513513514]\n",
            "7     [8, 98.19684447783621]     [8, 93.04054054054053]\n",
            "8     [9, 96.76934635612322]     [9, 91.82432432432432]\n",
            "9    [10, 96.91960931630354]    [10, 91.75675675675676]\n",
            "10   [11, 94.89105935386927]    [11, 88.91891891891892]\n",
            "11   [12, 94.96619083395943]    [12, 89.52702702702703]\n",
            "12   [13, 96.24342599549212]    [13, 90.81081081081082]\n",
            "13   [14, 87.60330578512396]    [14, 81.55405405405406]\n",
            "14    [15, 81.6679188580015]    [15, 79.12162162162161]\n",
            "15  [16, 49.812171299774604]   [16, 44.527027027027025]\n",
            "16   [17, 82.79489105935387]    [17, 78.64864864864865]\n",
            "17   [18, 74.45529676934636]    [18, 68.78378378378378]\n",
            "18   [19, 71.22464312546957]                 [19, 67.5]\n",
            "19   [20, 80.69120961682945]    [20, 76.28378378378379]\n",
            "20   [21, 91.51014274981218]    [21, 83.78378378378379]\n",
            "21   [22, 94.06461307287753]    [22, 88.04054054054053]\n",
            "22   [23, 94.89105935386927]    [23, 89.05405405405405]\n",
            "23   [24, 94.36513899323816]    [24, 90.06756756756756]\n",
            "24   [25, 96.46882043576258]    [25, 90.87837837837837]\n",
            "25   [26, 96.31855747558227]    [26, 90.67567567567568]\n",
            "26   [27, 96.54395191585274]    [27, 91.21621621621621]\n",
            "27    [28, 96.6190833959429]    [28, 91.28378378378378]\n",
            "28   [29, 96.91960931630354]    [29, 90.81081081081082]\n",
            "29   [30, 94.44027047332833]    [30, 87.43243243243242]\n",
            "30    [31, 93.6138241923366]    [31, 87.77027027027027]\n",
            "31   [32, 84.89857250187829]    [32, 78.24324324324324]\n",
            "32   [33, 78.28700225394441]    [33, 70.20270270270271]\n",
            "33   [34, 86.62659654395192]    [34, 80.74324324324324]\n",
            "34   [35, 53.26821938392187]    [35, 46.82432432432432]\n",
            "35  [36, 41.397445529676936]    [36, 38.58108108108108]\n",
            "36  [37, 41.397445529676936]    [37, 38.37837837837838]\n",
            "37  [38, 24.417731029301276]   [38, 22.094594594594593]\n",
            "38  [39, 25.018782870022537]    [39, 23.10810810810811]\n",
            "39  [40, 24.567993989481593]    [40, 22.22972972972973]\n",
            "40   [41, 24.94365138993238]   [41, 22.364864864864863]\n",
            "41    [42, 25.6198347107438]    [42, 23.31081081081081]\n",
            "42  [43, 25.845229151014276]   [43, 24.054054054054056]\n",
            "43  [44, 26.070623591284747]    [44, 24.45945945945946]\n",
            "44  [45, 26.145755071374904]   [45, 25.270270270270267]\n",
            "45  [46, 40.420736288504884]   [46, 36.891891891891895]\n",
            "46    [47, 38.6175807663411]    [47, 34.72972972972973]\n",
            "47   [48, 36.36363636363637]    [48, 33.71621621621622]\n",
            "48  [49, 28.549962434259957]    [49, 26.08108108108108]\n",
            "49    [50, 13.5236664162284]   [50, 13.243243243243244]\n",
            "50  [51, 12.697220135236664]   [51, 12.702702702702704]\n",
            "51  [52, 12.697220135236664]   [52, 12.702702702702704]\n",
            "52  [53, 12.697220135236664]   [53, 12.702702702702704]\n",
            "53  [54, 12.697220135236664]   [54, 12.702702702702704]\n",
            "54  [55, 12.697220135236664]   [55, 12.702702702702704]\n",
            "55  [56, 12.697220135236664]   [56, 12.702702702702704]\n",
            "56  [57, 12.697220135236664]   [57, 12.702702702702704]\n",
            "57  [58, 15.702479338842975]   [58, 14.797297297297296]\n",
            "58    [59, 13.5236664162284]   [59, 13.243243243243244]\n",
            "59  [60, 15.702479338842975]   [60, 14.797297297297296]\n",
            "60  [61, 15.702479338842975]   [61, 14.797297297297296]\n",
            "61  [62, 12.697220135236664]   [62, 12.702702702702704]\n",
            "62  [63, 12.697220135236664]   [63, 12.702702702702704]\n",
            "63  [64, 12.697220135236664]   [64, 12.702702702702704]\n",
            "64  [65, 12.697220135236664]   [65, 12.702702702702704]\n",
            "65  [66, 12.697220135236664]   [66, 12.702702702702704]\n",
            "66  [67, 12.697220135236664]   [67, 12.702702702702704]\n",
            "67    [68, 13.5236664162284]   [68, 13.243243243243244]\n",
            "68   [69, 16.15326821938392]    [69, 15.54054054054054]\n",
            "69  [70, 15.702479338842975]   [70, 14.797297297297296]\n",
            "70  [71, 12.697220135236664]   [71, 12.702702702702704]\n",
            "71  [72, 15.702479338842975]   [72, 14.797297297297296]\n",
            "72  [73, 12.697220135236664]   [73, 12.702702702702704]\n",
            "73   [74, 16.15326821938392]    [74, 15.54054054054054]\n",
            "74  [75, 13.974455296769348]   [75, 13.783783783783784]\n",
            "75  [76, 12.697220135236664]   [76, 12.702702702702704]\n",
            "76   [77, 16.15326821938392]    [77, 15.54054054054054]\n",
            "77  [78, 11.344853493613824]   [78, 12.027027027027028]\n",
            "78   [79, 16.15326821938392]    [79, 15.54054054054054]\n",
            "79   [80, 16.15326821938392]    [80, 15.54054054054054]\n",
            "80   [81, 16.15326821938392]    [81, 15.54054054054054]\n",
            "81  [82, 25.169045830202858]    [82, 22.22972972972973]\n",
            "82  [83, 15.702479338842975]   [83, 14.797297297297296]\n",
            "83  [84, 15.702479338842975]   [84, 14.797297297297296]\n",
            "84  [85, 15.702479338842975]   [85, 14.797297297297296]\n",
            "85  [86, 15.702479338842975]   [86, 14.797297297297296]\n",
            "86   [87, 16.15326821938392]    [87, 15.54054054054054]\n",
            "87   [88, 16.15326821938392]    [88, 15.54054054054054]\n",
            "88   [89, 16.15326821938392]    [89, 15.54054054054054]\n",
            "89   [90, 16.15326821938392]    [90, 15.54054054054054]\n",
            "90    [91, 13.5236664162284]   [91, 13.243243243243244]\n",
            "91  [92, 13.974455296769348]   [92, 13.783783783783784]\n",
            "92   [93, 16.15326821938392]    [93, 15.54054054054054]\n",
            "93   [94, 16.15326821938392]    [94, 15.54054054054054]\n",
            "94  [95, 15.702479338842975]   [95, 14.797297297297296]\n",
            "95  [96, 13.974455296769348]   [96, 13.783783783783784]\n",
            "96   [97, 16.15326821938392]    [97, 15.54054054054054]\n",
            "97   [98, 16.15326821938392]    [98, 15.54054054054054]\n",
            "98   [99, 16.15326821938392]    [99, 15.54054054054054]\n",
            "99  [100, 16.15326821938392]   [100, 15.54054054054054]\n",
            "   test data evaluation\n",
            "0             94.221411\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d persiannews -p 0.1 -w 0.1 -t 0.01 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}