{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/ec-gan_persiannews_p-10_w-10_t-50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QTBBIf6SIvQ",
        "outputId": "af3e80fb-7b8f-4d7b-e335-097396429e63"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "5651ba37-a996-40cc-8084-7617af0cf59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/075_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/076_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 78 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/076_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/077_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/077_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/078_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:13.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/078_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/079_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/079_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/080_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/080_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/081_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/081_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/082_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/082_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/083_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/083_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/084_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/084_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/085_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/085_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/086_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/086_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/087_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/087_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/088_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/088_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/089_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "evaluation\n",
            "evaluation : 215 / 1331 * 100 = 16.15326821938392 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "validation\n",
            "validate : 230 / 1480 * 100 = 15.54054054054054 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/089_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/090_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/090_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/091_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "92     93  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "92     93  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/091_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/092_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "92     93  13.523666\n",
            "93     94  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "92     93  13.243243\n",
            "93     94  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/092_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/093_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "92     93  13.523666\n",
            "93     94  13.523666\n",
            "94     95  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "92     93  13.243243\n",
            "93     94  13.243243\n",
            "94     95  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/093_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/094_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "92     93  13.523666\n",
            "93     94  13.523666\n",
            "94     95  13.523666\n",
            "95     96  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "92     93  13.243243\n",
            "93     94  13.243243\n",
            "94     95  13.243243\n",
            "95     96  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/094_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/095_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "92     93  13.523666\n",
            "93     94  13.523666\n",
            "94     95  13.523666\n",
            "95     96  13.523666\n",
            "96     97  13.523666\n",
            "evaluation\n",
            "evaluation : 180 / 1331 * 100 = 13.5236664162284 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "92     93  13.243243\n",
            "93     94  13.243243\n",
            "94     95  13.243243\n",
            "95     96  13.243243\n",
            "96     97  13.243243\n",
            "validation\n",
            "validate : 196 / 1480 * 100 = 13.243243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/095_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/096_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "92     93  13.523666\n",
            "93     94  13.523666\n",
            "94     95  13.523666\n",
            "95     96  13.523666\n",
            "96     97  13.523666\n",
            "97     98  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "92     93  13.243243\n",
            "93     94  13.243243\n",
            "94     95  13.243243\n",
            "95     96  13.243243\n",
            "96     97  13.243243\n",
            "97     98  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/096_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/097_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "92     93  13.523666\n",
            "93     94  13.523666\n",
            "94     95  13.523666\n",
            "95     96  13.523666\n",
            "96     97  13.523666\n",
            "97     98  13.974455\n",
            "98     99  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "92     93  13.243243\n",
            "93     94  13.243243\n",
            "94     95  13.243243\n",
            "95     96  13.243243\n",
            "96     97  13.243243\n",
            "97     98  13.783784\n",
            "98     99  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/097_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/098_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:41.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.422239\n",
            "11     12  98.497370\n",
            "12     13  98.647633\n",
            "13     14  76.709241\n",
            "14     15  67.392938\n",
            "15     16  89.481593\n",
            "16     17  97.220135\n",
            "17     18  97.220135\n",
            "18     19  93.613824\n",
            "19     20  95.717506\n",
            "20     21  96.093163\n",
            "21     22  96.543952\n",
            "22     23  80.841473\n",
            "23     24  91.284748\n",
            "24     25  91.359880\n",
            "25     26  76.483847\n",
            "26     27  67.693464\n",
            "27     28  76.934636\n",
            "28     29  91.885800\n",
            "29     30  93.914350\n",
            "30     31  95.642374\n",
            "31     32  90.833959\n",
            "32     33  84.973704\n",
            "33     34  93.989482\n",
            "34     35  87.528174\n",
            "35     36  84.673178\n",
            "36     37  80.691210\n",
            "37     38  85.048835\n",
            "38     39  85.499624\n",
            "39     40  84.823441\n",
            "40     41  70.623591\n",
            "41     42  73.478588\n",
            "42     43  73.929376\n",
            "43     44  74.004508\n",
            "44     45  60.706236\n",
            "45     46  59.203606\n",
            "46     47  57.250188\n",
            "47     48  56.574005\n",
            "48     49  60.781367\n",
            "49     50  52.592036\n",
            "50     51  73.027799\n",
            "51     52  72.351615\n",
            "52     53  73.478588\n",
            "53     54  74.229902\n",
            "54     55  76.333584\n",
            "55     56  72.501878\n",
            "56     57  71.149512\n",
            "57     58  74.154771\n",
            "58     59  75.206612\n",
            "59     60  76.333584\n",
            "60     61  76.408715\n",
            "61     62  86.326071\n",
            "62     63  70.623591\n",
            "63     64  62.283997\n",
            "64     65  74.305034\n",
            "65     66  74.154771\n",
            "66     67  74.229902\n",
            "67     68  76.333584\n",
            "68     69  76.333584\n",
            "69     70  76.333584\n",
            "70     71  76.333584\n",
            "71     72  76.333584\n",
            "72     73  76.333584\n",
            "73     74  76.408715\n",
            "74     75  75.882795\n",
            "75     76  12.697220\n",
            "76     77  16.153268\n",
            "77     78  16.153268\n",
            "78     79  16.153268\n",
            "79     80  16.153268\n",
            "80     81  16.153268\n",
            "81     82  13.523666\n",
            "82     83  13.523666\n",
            "83     84  13.974455\n",
            "84     85  13.974455\n",
            "85     86  13.974455\n",
            "86     87  13.974455\n",
            "87     88  13.974455\n",
            "88     89  13.974455\n",
            "89     90  16.153268\n",
            "90     91  16.153268\n",
            "91     92  13.523666\n",
            "92     93  13.523666\n",
            "93     94  13.523666\n",
            "94     95  13.523666\n",
            "95     96  13.523666\n",
            "96     97  13.523666\n",
            "97     98  13.974455\n",
            "98     99  13.974455\n",
            "99    100  13.974455\n",
            "evaluation\n",
            "evaluation : 186 / 1331 * 100 = 13.974455296769348 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  93.175676\n",
            "11     12  93.243243\n",
            "12     13  92.905405\n",
            "13     14  71.824324\n",
            "14     15  64.256757\n",
            "15     16  82.027027\n",
            "16     17  90.945946\n",
            "17     18  91.756757\n",
            "18     19  87.635135\n",
            "19     20  89.527027\n",
            "20     21  89.729730\n",
            "21     22  90.135135\n",
            "22     23  74.594595\n",
            "23     24  85.067568\n",
            "24     25  85.540541\n",
            "25     26  71.283784\n",
            "26     27  63.445946\n",
            "27     28  72.500000\n",
            "28     29  88.445946\n",
            "29     30  88.310811\n",
            "30     31  90.472973\n",
            "31     32  84.054054\n",
            "32     33  76.891892\n",
            "33     34  87.297297\n",
            "34     35  81.081081\n",
            "35     36  78.378378\n",
            "36     37  73.445946\n",
            "37     38  79.527027\n",
            "38     39  79.729730\n",
            "39     40  79.189189\n",
            "40     41  64.662162\n",
            "41     42  68.851351\n",
            "42     43  68.581081\n",
            "43     44  68.648649\n",
            "44     45  53.445946\n",
            "45     46  52.905405\n",
            "46     47  54.391892\n",
            "47     48  52.364865\n",
            "48     49  56.351351\n",
            "49     50  48.716216\n",
            "50     51  65.540541\n",
            "51     52  65.067568\n",
            "52     53  66.621622\n",
            "53     54  66.689189\n",
            "54     55  68.783784\n",
            "55     56  67.162162\n",
            "56     57  62.837838\n",
            "57     58  66.351351\n",
            "58     59  67.905405\n",
            "59     60  68.513514\n",
            "60     61  69.121622\n",
            "61     62  78.783784\n",
            "62     63  64.527027\n",
            "63     64  59.256757\n",
            "64     65  68.310811\n",
            "65     66  68.310811\n",
            "66     67  68.648649\n",
            "67     68  69.459459\n",
            "68     69  69.391892\n",
            "69     70  69.324324\n",
            "70     71  69.391892\n",
            "71     72  69.324324\n",
            "72     73  69.324324\n",
            "73     74  69.391892\n",
            "74     75  69.256757\n",
            "75     76  12.702703\n",
            "76     77  15.540541\n",
            "77     78  15.540541\n",
            "78     79  15.540541\n",
            "79     80  15.540541\n",
            "80     81  15.540541\n",
            "81     82  13.243243\n",
            "82     83  13.243243\n",
            "83     84  13.783784\n",
            "84     85  13.783784\n",
            "85     86  13.783784\n",
            "86     87  13.783784\n",
            "87     88  13.783784\n",
            "88     89  13.783784\n",
            "89     90  15.540541\n",
            "90     91  15.540541\n",
            "91     92  13.243243\n",
            "92     93  13.243243\n",
            "93     94  13.243243\n",
            "94     95  13.243243\n",
            "95     96  13.243243\n",
            "96     97  13.243243\n",
            "97     98  13.783784\n",
            "98     99  13.783784\n",
            "99    100  13.783784\n",
            "validation\n",
            "validate : 204 / 1480 * 100 = 13.783783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/098_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/099_persiannews_0.1_0.1_0.5.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.5/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  94.038929\n",
            "test\n",
            "test : 1546 / 1644 * 100 = 94.0389294403893 \n",
            "        train data evaluation validation data evaluation\n",
            "0      [1, 53.34335086401202]     [1, 50.74324324324324]\n",
            "1      [2, 82.04357625845229]     [2, 78.31081081081082]\n",
            "2      [3, 95.94290007513148]     [3, 93.17567567567568]\n",
            "3      [4, 96.09316303531179]     [4, 92.02702702702703]\n",
            "4      [5, 88.88054094665665]     [5, 83.78378378378379]\n",
            "5      [6, 95.56724267468068]     [6, 91.28378378378378]\n",
            "6      [7, 78.06160781367393]     [7, 73.44594594594595]\n",
            "7      [8, 98.19684447783621]     [8, 93.51351351351352]\n",
            "8      [9, 97.52066115702479]     [9, 91.95945945945945]\n",
            "9     [10, 97.37039819684448]    [10, 91.21621621621621]\n",
            "10    [11, 98.42223891810669]    [11, 93.17567567567568]\n",
            "11    [12, 98.49737039819685]    [12, 93.24324324324324]\n",
            "12    [13, 98.64763335837716]     [13, 92.9054054054054]\n",
            "13     [14, 76.7092411720511]    [14, 71.82432432432432]\n",
            "14    [15, 67.39293764087152]    [15, 64.25675675675676]\n",
            "15     [16, 89.4815927873779]    [16, 82.02702702702702]\n",
            "16    [17, 97.22013523666416]    [17, 90.94594594594595]\n",
            "17    [18, 97.22013523666416]    [18, 91.75675675675676]\n",
            "18     [19, 93.6138241923366]    [19, 87.63513513513513]\n",
            "19    [20, 95.71750563486101]    [20, 89.52702702702703]\n",
            "20    [21, 96.09316303531179]    [21, 89.72972972972974]\n",
            "21    [22, 96.54395191585274]    [22, 90.13513513513513]\n",
            "22    [23, 80.84147257700977]     [23, 74.5945945945946]\n",
            "23     [24, 91.2847483095417]    [24, 85.06756756756756]\n",
            "24    [25, 91.35987978963186]    [25, 85.54054054054055]\n",
            "25    [26, 76.48384673178062]    [26, 71.28378378378379]\n",
            "26    [27, 67.69346356123216]    [27, 63.44594594594595]\n",
            "27    [28, 76.93463561232157]                 [28, 72.5]\n",
            "28    [29, 91.88580015026297]    [29, 88.44594594594595]\n",
            "29    [30, 93.91435011269722]     [30, 88.3108108108108]\n",
            "30    [31, 95.64237415477085]    [31, 90.47297297297298]\n",
            "31    [32, 90.83395942900076]    [32, 84.05405405405405]\n",
            "32    [33, 84.97370398196844]     [33, 76.8918918918919]\n",
            "33    [34, 93.98948159278739]    [34, 87.29729729729729]\n",
            "34    [35, 87.52817430503382]    [35, 81.08108108108108]\n",
            "35    [36, 84.67317806160781]    [36, 78.37837837837837]\n",
            "36    [37, 80.69120961682945]    [37, 73.44594594594595]\n",
            "37     [38, 85.0488354620586]    [38, 79.52702702702703]\n",
            "38    [39, 85.49962434259955]    [39, 79.72972972972973]\n",
            "39    [40, 84.82344102178813]     [40, 79.1891891891892]\n",
            "40    [41, 70.62359128474831]    [41, 64.66216216216216]\n",
            "41     [42, 73.4785875281743]    [42, 68.85135135135135]\n",
            "42    [43, 73.92937640871526]    [43, 68.58108108108108]\n",
            "43     [44, 74.0045078888054]    [44, 68.64864864864865]\n",
            "44    [45, 60.70623591284748]    [45, 53.44594594594595]\n",
            "45    [46, 59.20360631104433]     [46, 52.9054054054054]\n",
            "46    [47, 57.25018782870023]   [47, 54.391891891891895]\n",
            "47    [48, 56.57400450788881]    [48, 52.36486486486487]\n",
            "48   [49, 60.781367392937646]   [49, 56.351351351351354]\n",
            "49    [50, 52.59203606311045]    [50, 48.71621621621622]\n",
            "50    [51, 73.02779864763336]    [51, 65.54054054054053]\n",
            "51    [52, 72.35161532682194]    [52, 65.06756756756756]\n",
            "52     [53, 73.4785875281743]    [53, 66.62162162162161]\n",
            "53    [54, 74.22990232907588]     [54, 66.6891891891892]\n",
            "54     [55, 76.3335837716003]    [55, 68.78378378378378]\n",
            "55    [56, 72.50187828700226]    [56, 67.16216216216216]\n",
            "56    [57, 71.14951164537942]    [57, 62.83783783783784]\n",
            "57    [58, 74.15477084898573]    [58, 66.35135135135135]\n",
            "58    [59, 75.20661157024794]     [59, 67.9054054054054]\n",
            "59     [60, 76.3335837716003]    [60, 68.51351351351352]\n",
            "60    [61, 76.40871525169047]    [61, 69.12162162162161]\n",
            "61    [62, 86.32607062359129]    [62, 78.78378378378378]\n",
            "62    [63, 70.62359128474831]    [63, 64.52702702702703]\n",
            "63     [64, 62.2839969947408]    [64, 59.25675675675676]\n",
            "64    [65, 74.30503380916605]     [65, 68.3108108108108]\n",
            "65    [66, 74.15477084898573]     [66, 68.3108108108108]\n",
            "66    [67, 74.22990232907588]    [67, 68.64864864864865]\n",
            "67     [68, 76.3335837716003]    [68, 69.45945945945947]\n",
            "68     [69, 76.3335837716003]    [69, 69.39189189189189]\n",
            "69     [70, 76.3335837716003]    [70, 69.32432432432432]\n",
            "70     [71, 76.3335837716003]    [71, 69.39189189189189]\n",
            "71     [72, 76.3335837716003]    [72, 69.32432432432432]\n",
            "72     [73, 76.3335837716003]    [73, 69.32432432432432]\n",
            "73    [74, 76.40871525169047]    [74, 69.39189189189189]\n",
            "74    [75, 75.88279489105936]    [75, 69.25675675675676]\n",
            "75   [76, 12.697220135236664]   [76, 12.702702702702704]\n",
            "76    [77, 16.15326821938392]    [77, 15.54054054054054]\n",
            "77    [78, 16.15326821938392]    [78, 15.54054054054054]\n",
            "78    [79, 16.15326821938392]    [79, 15.54054054054054]\n",
            "79    [80, 16.15326821938392]    [80, 15.54054054054054]\n",
            "80    [81, 16.15326821938392]    [81, 15.54054054054054]\n",
            "81     [82, 13.5236664162284]   [82, 13.243243243243244]\n",
            "82     [83, 13.5236664162284]   [83, 13.243243243243244]\n",
            "83   [84, 13.974455296769348]   [84, 13.783783783783784]\n",
            "84   [85, 13.974455296769348]   [85, 13.783783783783784]\n",
            "85   [86, 13.974455296769348]   [86, 13.783783783783784]\n",
            "86   [87, 13.974455296769348]   [87, 13.783783783783784]\n",
            "87   [88, 13.974455296769348]   [88, 13.783783783783784]\n",
            "88   [89, 13.974455296769348]   [89, 13.783783783783784]\n",
            "89    [90, 16.15326821938392]    [90, 15.54054054054054]\n",
            "90    [91, 16.15326821938392]    [91, 15.54054054054054]\n",
            "91     [92, 13.5236664162284]   [92, 13.243243243243244]\n",
            "92     [93, 13.5236664162284]   [93, 13.243243243243244]\n",
            "93     [94, 13.5236664162284]   [94, 13.243243243243244]\n",
            "94     [95, 13.5236664162284]   [95, 13.243243243243244]\n",
            "95     [96, 13.5236664162284]   [96, 13.243243243243244]\n",
            "96     [97, 13.5236664162284]   [97, 13.243243243243244]\n",
            "97   [98, 13.974455296769348]   [98, 13.783783783783784]\n",
            "98   [99, 13.974455296769348]   [99, 13.783783783783784]\n",
            "99  [100, 13.974455296769348]  [100, 13.783783783783784]\n",
            "   test data evaluation\n",
            "0             94.038929\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d persiannews -p 0.1 -w 0.1 -t 0.5 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}