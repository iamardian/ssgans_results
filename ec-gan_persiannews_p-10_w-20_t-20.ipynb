{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/ec-gan_persiannews_p-10_w-20_t-20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py4wTzEeXNpk",
        "outputId": "923de363-6349-462e-c8f0-32ef8f4da071"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kVzghhQznhMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef965d3-fe64-4699-b129-3272dc9f434e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "evaluation\n",
            "evaluation : 318 / 1331 * 100 = 23.89181066867017 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "validation\n",
            "validate : 340 / 1480 * 100 = 22.972972972972975 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/076_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/077_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "evaluation\n",
            "evaluation : 345 / 1331 * 100 = 25.920360631104433 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/077_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/078_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "evaluation\n",
            "evaluation : 345 / 1331 * 100 = 25.920360631104433 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/078_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/079_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "evaluation\n",
            "evaluation : 304 / 1331 * 100 = 22.839969947407965 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "validation\n",
            "validate : 332 / 1480 * 100 = 22.432432432432435 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/079_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/080_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "evaluation\n",
            "evaluation : 303 / 1331 * 100 = 22.764838467317805 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "validation\n",
            "validate : 332 / 1480 * 100 = 22.432432432432435 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/080_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/081_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/081_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/082_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "evaluation\n",
            "evaluation : 317 / 1331 * 100 = 23.816679188580014 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "validation\n",
            "validate : 340 / 1480 * 100 = 22.972972972972975 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/082_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/083_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/083_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/084_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "evaluation\n",
            "evaluation : 317 / 1331 * 100 = 23.816679188580014 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "validation\n",
            "validate : 340 / 1480 * 100 = 22.972972972972975 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/084_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/085_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/085_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/086_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "evaluation\n",
            "evaluation : 339 / 1331 * 100 = 25.469571750563485 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "validation\n",
            "validate : 364 / 1480 * 100 = 24.594594594594597 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/086_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/087_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "evaluation\n",
            "evaluation : 317 / 1331 * 100 = 23.816679188580014 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "validation\n",
            "validate : 340 / 1480 * 100 = 22.972972972972975 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/087_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/088_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "evaluation\n",
            "evaluation : 339 / 1331 * 100 = 25.469571750563485 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "validation\n",
            "validate : 364 / 1480 * 100 = 24.594594594594597 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/088_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/089_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "evaluation\n",
            "evaluation : 339 / 1331 * 100 = 25.469571750563485 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "validation\n",
            "validate : 364 / 1480 * 100 = 24.594594594594597 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/089_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/090_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/090_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/091_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "92     93  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "92     93  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/091_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/092_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "92     93  25.845229\n",
            "93     94  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "92     93  24.391892\n",
            "93     94  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/092_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/093_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "92     93  25.845229\n",
            "93     94  25.845229\n",
            "94     95  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "92     93  24.391892\n",
            "93     94  24.391892\n",
            "94     95  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/093_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/094_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "92     93  25.845229\n",
            "93     94  25.845229\n",
            "94     95  25.845229\n",
            "95     96  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "92     93  24.391892\n",
            "93     94  24.391892\n",
            "94     95  24.391892\n",
            "95     96  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/094_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/095_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "92     93  25.845229\n",
            "93     94  25.845229\n",
            "94     95  25.845229\n",
            "95     96  25.845229\n",
            "96     97  25.469572\n",
            "evaluation\n",
            "evaluation : 339 / 1331 * 100 = 25.469571750563485 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "92     93  24.391892\n",
            "93     94  24.391892\n",
            "94     95  24.391892\n",
            "95     96  24.391892\n",
            "96     97  24.594595\n",
            "validation\n",
            "validate : 364 / 1480 * 100 = 24.594594594594597 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/095_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/096_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "92     93  25.845229\n",
            "93     94  25.845229\n",
            "94     95  25.845229\n",
            "95     96  25.845229\n",
            "96     97  25.469572\n",
            "97     98  25.469572\n",
            "evaluation\n",
            "evaluation : 339 / 1331 * 100 = 25.469571750563485 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "92     93  24.391892\n",
            "93     94  24.391892\n",
            "94     95  24.391892\n",
            "95     96  24.391892\n",
            "96     97  24.594595\n",
            "97     98  24.594595\n",
            "validation\n",
            "validate : 364 / 1480 * 100 = 24.594594594594597 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/096_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/097_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:28.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:56.\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "92     93  25.845229\n",
            "93     94  25.845229\n",
            "94     95  25.845229\n",
            "95     96  25.845229\n",
            "96     97  25.469572\n",
            "97     98  25.469572\n",
            "98     99  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "92     93  24.391892\n",
            "93     94  24.391892\n",
            "94     95  24.391892\n",
            "95     96  24.391892\n",
            "96     97  24.594595\n",
            "97     98  24.594595\n",
            "98     99  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/097_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/098_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "  Batch    10  of     42.    Elapsed: 0:00:14.\n",
            "  Batch    20  of     42.    Elapsed: 0:00:27.\n",
            "  Batch    30  of     42.    Elapsed: 0:00:42.\n",
            "  Batch    40  of     42.    Elapsed: 0:00:55.\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch        acc\n",
            "0       1  53.343351\n",
            "1       2  82.043576\n",
            "2       3  95.942900\n",
            "3       4  96.093163\n",
            "4       5  88.880541\n",
            "5       6  95.567243\n",
            "6       7  78.061608\n",
            "7       8  98.196844\n",
            "8       9  97.520661\n",
            "9      10  97.370398\n",
            "10     11  98.196844\n",
            "11     12  91.209617\n",
            "12     13  90.157776\n",
            "13     14  91.735537\n",
            "14     15  97.295267\n",
            "15     16  97.520661\n",
            "16     17  83.771600\n",
            "17     18  68.670173\n",
            "18     19  64.162284\n",
            "19     20  95.341848\n",
            "20     21  95.717506\n",
            "21     22  95.416980\n",
            "22     23  95.492111\n",
            "23     24  96.243426\n",
            "24     25  80.691210\n",
            "25     26  88.429752\n",
            "26     27  89.406461\n",
            "27     28  71.224643\n",
            "28     29  87.302780\n",
            "29     30  88.880541\n",
            "30     31  88.504884\n",
            "31     32  88.580015\n",
            "32     33  62.208866\n",
            "33     34  85.123967\n",
            "34     35  50.187829\n",
            "35     36  24.868520\n",
            "36     37  26.671675\n",
            "37     38  26.746807\n",
            "38     39  24.943651\n",
            "39     40  24.943651\n",
            "40     41  26.746807\n",
            "41     42  13.523666\n",
            "42     43  23.666416\n",
            "43     44  21.337340\n",
            "44     45  24.492863\n",
            "45     46  25.770098\n",
            "46     47  25.770098\n",
            "47     48  24.117205\n",
            "48     49  26.145755\n",
            "49     50  26.145755\n",
            "50     51  24.117205\n",
            "51     52  24.117205\n",
            "52     53  24.117205\n",
            "53     54  24.042074\n",
            "54     55  24.042074\n",
            "55     56  24.042074\n",
            "56     57  24.042074\n",
            "57     58  24.042074\n",
            "58     59  24.042074\n",
            "59     60  24.042074\n",
            "60     61  24.042074\n",
            "61     62  24.042074\n",
            "62     63  24.042074\n",
            "63     64  24.042074\n",
            "64     65  24.042074\n",
            "65     66  24.042074\n",
            "66     67  24.042074\n",
            "67     68  25.694966\n",
            "68     69  25.770098\n",
            "69     70  23.140496\n",
            "70     71  26.220887\n",
            "71     72  23.065364\n",
            "72     73  26.145755\n",
            "73     74  24.117205\n",
            "74     75  24.117205\n",
            "75     76  23.891811\n",
            "76     77  23.891811\n",
            "77     78  23.891811\n",
            "78     79  25.920361\n",
            "79     80  25.920361\n",
            "80     81  22.839970\n",
            "81     82  22.764838\n",
            "82     83  25.845229\n",
            "83     84  23.816679\n",
            "84     85  25.845229\n",
            "85     86  23.816679\n",
            "86     87  25.845229\n",
            "87     88  25.469572\n",
            "88     89  23.816679\n",
            "89     90  25.469572\n",
            "90     91  25.469572\n",
            "91     92  25.845229\n",
            "92     93  25.845229\n",
            "93     94  25.845229\n",
            "94     95  25.845229\n",
            "95     96  25.845229\n",
            "96     97  25.469572\n",
            "97     98  25.469572\n",
            "98     99  25.845229\n",
            "99    100  25.845229\n",
            "evaluation\n",
            "evaluation : 344 / 1331 * 100 = 25.845229151014276 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  50.743243\n",
            "1       2  78.310811\n",
            "2       3  93.175676\n",
            "3       4  92.027027\n",
            "4       5  83.783784\n",
            "5       6  91.283784\n",
            "6       7  73.445946\n",
            "7       8  93.513514\n",
            "8       9  91.959459\n",
            "9      10  91.216216\n",
            "10     11  92.635135\n",
            "11     12  84.324324\n",
            "12     13  84.459459\n",
            "13     14  84.256757\n",
            "14     15  90.743243\n",
            "15     16  90.743243\n",
            "16     17  77.297297\n",
            "17     18  65.270270\n",
            "18     19  61.689189\n",
            "19     20  88.986486\n",
            "20     21  89.256757\n",
            "21     22  88.783784\n",
            "22     23  89.527027\n",
            "23     24  89.391892\n",
            "24     25  74.459459\n",
            "25     26  80.878378\n",
            "26     27  83.310811\n",
            "27     28  66.554054\n",
            "28     29  81.013514\n",
            "29     30  83.581081\n",
            "30     31  82.972973\n",
            "31     32  82.635135\n",
            "32     33  56.959459\n",
            "33     34  79.391892\n",
            "34     35  45.743243\n",
            "35     36  24.662162\n",
            "36     37  26.283784\n",
            "37     38  25.945946\n",
            "38     39  24.662162\n",
            "39     40  24.662162\n",
            "40     41  26.283784\n",
            "41     42  13.243243\n",
            "42     43  22.432432\n",
            "43     44  19.324324\n",
            "44     45  23.243243\n",
            "45     46  24.864865\n",
            "46     47  24.864865\n",
            "47     48  23.243243\n",
            "48     49  24.662162\n",
            "49     50  24.662162\n",
            "50     51  23.243243\n",
            "51     52  23.310811\n",
            "52     53  23.310811\n",
            "53     54  23.310811\n",
            "54     55  23.310811\n",
            "55     56  23.310811\n",
            "56     57  23.310811\n",
            "57     58  23.310811\n",
            "58     59  23.310811\n",
            "59     60  23.310811\n",
            "60     61  23.310811\n",
            "61     62  23.310811\n",
            "62     63  23.310811\n",
            "63     64  23.310811\n",
            "64     65  23.310811\n",
            "65     66  23.310811\n",
            "66     67  23.310811\n",
            "67     68  24.932432\n",
            "68     69  24.932432\n",
            "69     70  22.770270\n",
            "70     71  24.729730\n",
            "71     72  22.770270\n",
            "72     73  24.729730\n",
            "73     74  23.310811\n",
            "74     75  23.310811\n",
            "75     76  22.972973\n",
            "76     77  22.972973\n",
            "77     78  22.972973\n",
            "78     79  24.391892\n",
            "79     80  24.391892\n",
            "80     81  22.432432\n",
            "81     82  22.432432\n",
            "82     83  24.391892\n",
            "83     84  22.972973\n",
            "84     85  24.391892\n",
            "85     86  22.972973\n",
            "86     87  24.391892\n",
            "87     88  24.594595\n",
            "88     89  22.972973\n",
            "89     90  24.594595\n",
            "90     91  24.594595\n",
            "91     92  24.391892\n",
            "92     93  24.391892\n",
            "93     94  24.391892\n",
            "94     95  24.391892\n",
            "95     96  24.391892\n",
            "96     97  24.594595\n",
            "97     98  24.594595\n",
            "98     99  24.391892\n",
            "99    100  24.391892\n",
            "validation\n",
            "validate : 361 / 1480 * 100 = 24.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "best_model_accuracy : 93.51351351351352\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/098_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/099_persiannews_0.1_0.2_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.2|0.2/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  94.038929\n",
            "test\n",
            "test : 1546 / 1644 * 100 = 94.0389294403893 \n",
            "        train data evaluation validation data evaluation\n",
            "0      [1, 53.34335086401202]     [1, 50.74324324324324]\n",
            "1      [2, 82.04357625845229]     [2, 78.31081081081082]\n",
            "2      [3, 95.94290007513148]     [3, 93.17567567567568]\n",
            "3      [4, 96.09316303531179]     [4, 92.02702702702703]\n",
            "4      [5, 88.88054094665665]     [5, 83.78378378378379]\n",
            "5      [6, 95.56724267468068]     [6, 91.28378378378378]\n",
            "6      [7, 78.06160781367393]     [7, 73.44594594594595]\n",
            "7      [8, 98.19684447783621]     [8, 93.51351351351352]\n",
            "8      [9, 97.52066115702479]     [9, 91.95945945945945]\n",
            "9     [10, 97.37039819684448]    [10, 91.21621621621621]\n",
            "10    [11, 98.19684447783621]    [11, 92.63513513513514]\n",
            "11    [12, 91.20961682945155]    [12, 84.32432432432432]\n",
            "12    [13, 90.15777610818934]    [13, 84.45945945945947]\n",
            "13    [14, 91.73553719008265]    [14, 84.25675675675676]\n",
            "14    [15, 97.29526671675433]    [15, 90.74324324324324]\n",
            "15    [16, 97.52066115702479]    [16, 90.74324324324324]\n",
            "16    [17, 83.77160030052592]    [17, 77.29729729729729]\n",
            "17    [18, 68.67017280240421]    [18, 65.27027027027027]\n",
            "18    [19, 64.16228399699474]   [19, 61.689189189189186]\n",
            "19    [20, 95.34184823441022]    [20, 88.98648648648648]\n",
            "20    [21, 95.71750563486101]    [21, 89.25675675675676]\n",
            "21    [22, 95.41697971450037]    [22, 88.78378378378379]\n",
            "22    [23, 95.49211119459054]    [23, 89.52702702702703]\n",
            "23    [24, 96.24342599549212]     [24, 89.3918918918919]\n",
            "24    [25, 80.69120961682945]    [25, 74.45945945945947]\n",
            "25    [26, 88.42975206611571]    [26, 80.87837837837839]\n",
            "26    [27, 89.40646130728776]     [27, 83.3108108108108]\n",
            "27    [28, 71.22464312546957]    [28, 66.55405405405406]\n",
            "28    [29, 87.30277986476334]    [29, 81.01351351351352]\n",
            "29    [30, 88.88054094665665]    [30, 83.58108108108108]\n",
            "30    [31, 88.50488354620586]    [31, 82.97297297297297]\n",
            "31    [32, 88.58001502629602]    [32, 82.63513513513514]\n",
            "32    [33, 62.20886551465063]    [33, 56.95945945945946]\n",
            "33    [34, 85.12396694214877]     [34, 79.3918918918919]\n",
            "34    [35, 50.18782870022539]    [35, 45.74324324324324]\n",
            "35   [36, 24.868519909842224]    [36, 24.66216216216216]\n",
            "36    [37, 26.67167543200601]   [37, 26.283783783783782]\n",
            "37   [38, 26.746806912096165]   [38, 25.945945945945947]\n",
            "38    [39, 24.94365138993238]    [39, 24.66216216216216]\n",
            "39    [40, 24.94365138993238]    [40, 24.66216216216216]\n",
            "40   [41, 26.746806912096165]   [41, 26.283783783783782]\n",
            "41     [42, 13.5236664162284]   [42, 13.243243243243244]\n",
            "42     [43, 23.6664162283997]   [43, 22.432432432432435]\n",
            "43   [44, 21.337340345604808]   [44, 19.324324324324323]\n",
            "44   [45, 24.492862509391436]   [45, 23.243243243243246]\n",
            "45    [46, 25.77009767092412]   [46, 24.864864864864867]\n",
            "46    [47, 25.77009767092412]   [47, 24.864864864864867]\n",
            "47   [48, 24.117205108940645]   [48, 23.243243243243246]\n",
            "48   [49, 26.145755071374904]    [49, 24.66216216216216]\n",
            "49   [50, 26.145755071374904]    [50, 24.66216216216216]\n",
            "50   [51, 24.117205108940645]   [51, 23.243243243243246]\n",
            "51   [52, 24.117205108940645]    [52, 23.31081081081081]\n",
            "52   [53, 24.117205108940645]    [53, 23.31081081081081]\n",
            "53    [54, 24.04207362885049]    [54, 23.31081081081081]\n",
            "54    [55, 24.04207362885049]    [55, 23.31081081081081]\n",
            "55    [56, 24.04207362885049]    [56, 23.31081081081081]\n",
            "56    [57, 24.04207362885049]    [57, 23.31081081081081]\n",
            "57    [58, 24.04207362885049]    [58, 23.31081081081081]\n",
            "58    [59, 24.04207362885049]    [59, 23.31081081081081]\n",
            "59    [60, 24.04207362885049]    [60, 23.31081081081081]\n",
            "60    [61, 24.04207362885049]    [61, 23.31081081081081]\n",
            "61    [62, 24.04207362885049]    [62, 23.31081081081081]\n",
            "62    [63, 24.04207362885049]    [63, 23.31081081081081]\n",
            "63    [64, 24.04207362885049]    [64, 23.31081081081081]\n",
            "64    [65, 24.04207362885049]    [65, 23.31081081081081]\n",
            "65    [66, 24.04207362885049]    [66, 23.31081081081081]\n",
            "66    [67, 24.04207362885049]    [67, 23.31081081081081]\n",
            "67   [68, 25.694966190833963]    [68, 24.93243243243243]\n",
            "68    [69, 25.77009767092412]    [69, 24.93243243243243]\n",
            "69   [70, 23.140495867768596]    [70, 22.77027027027027]\n",
            "70    [71, 26.22088655146506]    [71, 24.72972972972973]\n",
            "71   [72, 23.065364387678436]    [72, 22.77027027027027]\n",
            "72   [73, 26.145755071374904]    [73, 24.72972972972973]\n",
            "73   [74, 24.117205108940645]    [74, 23.31081081081081]\n",
            "74   [75, 24.117205108940645]    [75, 23.31081081081081]\n",
            "75    [76, 23.89181066867017]   [76, 22.972972972972975]\n",
            "76    [77, 23.89181066867017]   [77, 22.972972972972975]\n",
            "77    [78, 23.89181066867017]   [78, 22.972972972972975]\n",
            "78   [79, 25.920360631104433]    [79, 24.39189189189189]\n",
            "79   [80, 25.920360631104433]    [80, 24.39189189189189]\n",
            "80   [81, 22.839969947407965]   [81, 22.432432432432435]\n",
            "81   [82, 22.764838467317805]   [82, 22.432432432432435]\n",
            "82   [83, 25.845229151014276]    [83, 24.39189189189189]\n",
            "83   [84, 23.816679188580014]   [84, 22.972972972972975]\n",
            "84   [85, 25.845229151014276]    [85, 24.39189189189189]\n",
            "85   [86, 23.816679188580014]   [86, 22.972972972972975]\n",
            "86   [87, 25.845229151014276]    [87, 24.39189189189189]\n",
            "87   [88, 25.469571750563485]   [88, 24.594594594594597]\n",
            "88   [89, 23.816679188580014]   [89, 22.972972972972975]\n",
            "89   [90, 25.469571750563485]   [90, 24.594594594594597]\n",
            "90   [91, 25.469571750563485]   [91, 24.594594594594597]\n",
            "91   [92, 25.845229151014276]    [92, 24.39189189189189]\n",
            "92   [93, 25.845229151014276]    [93, 24.39189189189189]\n",
            "93   [94, 25.845229151014276]    [94, 24.39189189189189]\n",
            "94   [95, 25.845229151014276]    [95, 24.39189189189189]\n",
            "95   [96, 25.845229151014276]    [96, 24.39189189189189]\n",
            "96   [97, 25.469571750563485]   [97, 24.594594594594597]\n",
            "97   [98, 25.469571750563485]   [98, 24.594594594594597]\n",
            "98   [99, 25.845229151014276]    [99, 24.39189189189189]\n",
            "99  [100, 25.845229151014276]   [100, 24.39189189189189]\n",
            "   test data evaluation\n",
            "0             94.038929\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d persiannews -p 0.1 -w 0.2 -t 0.2 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "dir_path = \"/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.1|0.1|0.2/\"\n",
        "filelist = sorted(filter(os.path.isfile, glob.glob(dir_path + '*')))\n",
        "for f in reversed(range(len(filelist))):\n",
        "  print(filelist[f])"
      ],
      "metadata": {
        "id": "A9-B8L84CMnW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HCN-QCzF48lB"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}