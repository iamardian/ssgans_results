{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/ec-gan_persiannews_p-1_w-1_t-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QTBBIf6SIvQ",
        "outputId": "63b58b36-5f59-4e88-daec-3c71a270912b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "9a718f4b-11cc-496a-98f5-bcea0d7142c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "validation\n",
            "validate : 1340 / 1480 * 100 = 90.54054054054053 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/075_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/076_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 78 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "validation\n",
            "validate : 1334 / 1480 * 100 = 90.13513513513513 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/076_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/077_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "validation\n",
            "validate : 1333 / 1480 * 100 = 90.06756756756756 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/077_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/078_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "validation\n",
            "validate : 1323 / 1480 * 100 = 89.3918918918919 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/078_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/079_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "validation\n",
            "validate : 1330 / 1480 * 100 = 89.86486486486487 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/079_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/080_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "validation\n",
            "validate : 1337 / 1480 * 100 = 90.33783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/080_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/081_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "validation\n",
            "validate : 1340 / 1480 * 100 = 90.54054054054053 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/081_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/082_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "validation\n",
            "validate : 1345 / 1480 * 100 = 90.87837837837837 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/082_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/083_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "validation\n",
            "validate : 1345 / 1480 * 100 = 90.87837837837837 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/083_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/084_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "validation\n",
            "validate : 1340 / 1480 * 100 = 90.54054054054053 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/084_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/085_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "validation\n",
            "validate : 1341 / 1480 * 100 = 90.60810810810811 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/085_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/086_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "validation\n",
            "validate : 1337 / 1480 * 100 = 90.33783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/086_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/087_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "validation\n",
            "validate : 1339 / 1480 * 100 = 90.47297297297298 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/087_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/088_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "validation\n",
            "validate : 1332 / 1480 * 100 = 90.0 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/088_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/089_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "validation\n",
            "validate : 1334 / 1480 * 100 = 90.13513513513513 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/089_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/090_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "validation\n",
            "validate : 1332 / 1480 * 100 = 90.0 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/090_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/091_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "92     93  89.729730\n",
            "validation\n",
            "validate : 1328 / 1480 * 100 = 89.72972972972974 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/091_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/092_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "92     93  89.729730\n",
            "93     94  89.864865\n",
            "validation\n",
            "validate : 1330 / 1480 * 100 = 89.86486486486487 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/092_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/093_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "92     93  89.729730\n",
            "93     94  89.864865\n",
            "94     95  89.864865\n",
            "validation\n",
            "validate : 1330 / 1480 * 100 = 89.86486486486487 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/093_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/094_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "92     93  89.729730\n",
            "93     94  89.864865\n",
            "94     95  89.864865\n",
            "95     96  89.932432\n",
            "validation\n",
            "validate : 1331 / 1480 * 100 = 89.93243243243244 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/094_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/095_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "96     97  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "92     93  89.729730\n",
            "93     94  89.864865\n",
            "94     95  89.864865\n",
            "95     96  89.932432\n",
            "96     97  90.135135\n",
            "validation\n",
            "validate : 1334 / 1480 * 100 = 90.13513513513513 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/095_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/096_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "96     97  100.000000\n",
            "97     98  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "92     93  89.729730\n",
            "93     94  89.864865\n",
            "94     95  89.864865\n",
            "95     96  89.932432\n",
            "96     97  90.135135\n",
            "97     98  90.067568\n",
            "validation\n",
            "validate : 1333 / 1480 * 100 = 90.06756756756756 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/096_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/097_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "96     97  100.000000\n",
            "97     98  100.000000\n",
            "98     99  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "92     93  89.729730\n",
            "93     94  89.864865\n",
            "94     95  89.864865\n",
            "95     96  89.932432\n",
            "96     97  90.135135\n",
            "97     98  90.067568\n",
            "98     99  90.337838\n",
            "validation\n",
            "validate : 1337 / 1480 * 100 = 90.33783783783784 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/097_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/098_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   18.796992\n",
            "1       2   15.037594\n",
            "2       3   15.037594\n",
            "3       4   30.075188\n",
            "4       5   30.827068\n",
            "5       6   31.578947\n",
            "6       7   42.857143\n",
            "7       8   57.142857\n",
            "8       9   60.150376\n",
            "9      10   80.451128\n",
            "10     11   86.466165\n",
            "11     12   86.466165\n",
            "12     13   86.466165\n",
            "13     14   95.488722\n",
            "14     15   95.488722\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "96     97  100.000000\n",
            "97     98  100.000000\n",
            "98     99  100.000000\n",
            "99    100  100.000000\n",
            "evaluation\n",
            "evaluation : 133 / 133 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  20.675676\n",
            "1       2  14.797297\n",
            "2       3  14.797297\n",
            "3       4  26.756757\n",
            "4       5  27.770270\n",
            "5       6  28.851351\n",
            "6       7  35.878378\n",
            "7       8  44.864865\n",
            "8       9  46.689189\n",
            "9      10  60.135135\n",
            "10     11  70.405405\n",
            "11     12  69.324324\n",
            "12     13  70.000000\n",
            "13     14  81.418919\n",
            "14     15  82.364865\n",
            "15     16  88.310811\n",
            "16     17  87.027027\n",
            "17     18  90.337838\n",
            "18     19  90.270270\n",
            "19     20  90.540541\n",
            "20     21  90.405405\n",
            "21     22  90.743243\n",
            "22     23  91.418919\n",
            "23     24  91.081081\n",
            "24     25  91.418919\n",
            "25     26  91.216216\n",
            "26     27  91.081081\n",
            "27     28  91.013514\n",
            "28     29  91.081081\n",
            "29     30  91.351351\n",
            "30     31  91.351351\n",
            "31     32  91.554054\n",
            "32     33  91.418919\n",
            "33     34  91.351351\n",
            "34     35  91.216216\n",
            "35     36  91.283784\n",
            "36     37  91.418919\n",
            "37     38  91.148649\n",
            "38     39  91.418919\n",
            "39     40  91.418919\n",
            "40     41  91.148649\n",
            "41     42  91.148649\n",
            "42     43  91.418919\n",
            "43     44  90.878378\n",
            "44     45  91.283784\n",
            "45     46  90.945946\n",
            "46     47  90.945946\n",
            "47     48  91.013514\n",
            "48     49  90.608108\n",
            "49     50  91.013514\n",
            "50     51  90.743243\n",
            "51     52  91.013514\n",
            "52     53  91.013514\n",
            "53     54  90.810811\n",
            "54     55  90.270270\n",
            "55     56  90.608108\n",
            "56     57  90.067568\n",
            "57     58  90.810811\n",
            "58     59  90.743243\n",
            "59     60  90.337838\n",
            "60     61  90.472973\n",
            "61     62  90.135135\n",
            "62     63  90.743243\n",
            "63     64  90.135135\n",
            "64     65  90.472973\n",
            "65     66  90.067568\n",
            "66     67  90.270270\n",
            "67     68  90.202703\n",
            "68     69  89.932432\n",
            "69     70  90.270270\n",
            "70     71  89.932432\n",
            "71     72  89.729730\n",
            "72     73  89.594595\n",
            "73     74  89.662162\n",
            "74     75  90.337838\n",
            "75     76  89.864865\n",
            "76     77  90.540541\n",
            "77     78  90.135135\n",
            "78     79  90.067568\n",
            "79     80  89.391892\n",
            "80     81  89.864865\n",
            "81     82  90.337838\n",
            "82     83  90.540541\n",
            "83     84  90.878378\n",
            "84     85  90.878378\n",
            "85     86  90.540541\n",
            "86     87  90.608108\n",
            "87     88  90.337838\n",
            "88     89  90.472973\n",
            "89     90  90.000000\n",
            "90     91  90.135135\n",
            "91     92  90.000000\n",
            "92     93  89.729730\n",
            "93     94  89.864865\n",
            "94     95  89.864865\n",
            "95     96  89.932432\n",
            "96     97  90.135135\n",
            "97     98  90.067568\n",
            "98     99  90.337838\n",
            "99    100  90.270270\n",
            "validation\n",
            "validate : 1336 / 1480 * 100 = 90.27027027027027 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "best_model_accuracy : 91.55405405405406\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/098_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/099_persiannews_0.01_0.01_0.01.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.01|0.01|0.01/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:374: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  92.396594\n",
            "test\n",
            "test : 1519 / 1644 * 100 = 92.39659367396594 \n",
            "      train data evaluation validation data evaluation\n",
            "0   [1, 18.796992481203006]    [1, 20.675675675675677]\n",
            "1   [2, 15.037593984962406]    [2, 14.797297297297296]\n",
            "2   [3, 15.037593984962406]    [3, 14.797297297297296]\n",
            "3   [4, 30.075187969924812]    [4, 26.756756756756754]\n",
            "4    [5, 30.82706766917293]     [5, 27.77027027027027]\n",
            "..                      ...                        ...\n",
            "95              [96, 100.0]    [96, 89.93243243243244]\n",
            "96              [97, 100.0]    [97, 90.13513513513513]\n",
            "97              [98, 100.0]    [98, 90.06756756756756]\n",
            "98              [99, 100.0]    [99, 90.33783783783784]\n",
            "99             [100, 100.0]   [100, 90.27027027027027]\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "   test data evaluation\n",
            "0             92.396594\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d persiannews -p 0.01 -w 0.01 -t 0.01 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}