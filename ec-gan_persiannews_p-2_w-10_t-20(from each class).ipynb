{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/ec-gan_persiannews_p-2_w-10_t-20(from%20each%20class).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8x09BEF8GDv",
        "outputId": "748ea121-597c-4d66-e518-93b9be83b840"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "20f12ffc-dfad-4014-a8f2-f5df754e6ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "validation\n",
            "validate : 1391 / 1480 * 100 = 93.98648648648648 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 93.98648648648648\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/075_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/076_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 78 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "validation\n",
            "validate : 1394 / 1480 * 100 = 94.1891891891892 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 93.98648648648648\n",
            "Best model Saved\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/076_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/077_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 79 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "validation\n",
            "validate : 1387 / 1480 * 100 = 93.71621621621622 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/077_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/078_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 80 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "validation\n",
            "validate : 1384 / 1480 * 100 = 93.51351351351352 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/078_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/079_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 81 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "validation\n",
            "validate : 1381 / 1480 * 100 = 93.3108108108108 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/079_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/080_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 82 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "validation\n",
            "validate : 1382 / 1480 * 100 = 93.37837837837839 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/080_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/081_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 83 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "validation\n",
            "validate : 1380 / 1480 * 100 = 93.24324324324324 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/081_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/082_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 84 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "validation\n",
            "validate : 1381 / 1480 * 100 = 93.3108108108108 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/082_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/083_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 85 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "validation\n",
            "validate : 1383 / 1480 * 100 = 93.44594594594594 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/083_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/084_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 86 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "validation\n",
            "validate : 1384 / 1480 * 100 = 93.51351351351352 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/084_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/085_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 87 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "validation\n",
            "validate : 1380 / 1480 * 100 = 93.24324324324324 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/085_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/086_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 88 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "validation\n",
            "validate : 1378 / 1480 * 100 = 93.10810810810811 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/086_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/087_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 89 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "validation\n",
            "validate : 1379 / 1480 * 100 = 93.17567567567568 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/087_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/088_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 90 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "validation\n",
            "validate : 1383 / 1480 * 100 = 93.44594594594594 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/088_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/089_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 91 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "validation\n",
            "validate : 1381 / 1480 * 100 = 93.3108108108108 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/089_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/090_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 92 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "validation\n",
            "validate : 1389 / 1480 * 100 = 93.85135135135135 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/090_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/091_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 93 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "92     93  93.986486\n",
            "validation\n",
            "validate : 1391 / 1480 * 100 = 93.98648648648648 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/091_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/092_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 94 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "92     93  93.986486\n",
            "93     94  93.986486\n",
            "validation\n",
            "validate : 1391 / 1480 * 100 = 93.98648648648648 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/092_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/093_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 95 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "92     93  93.986486\n",
            "93     94  93.986486\n",
            "94     95  94.121622\n",
            "validation\n",
            "validate : 1393 / 1480 * 100 = 94.12162162162161 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/093_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/094_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 96 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "92     93  93.986486\n",
            "93     94  93.986486\n",
            "94     95  94.121622\n",
            "95     96  93.986486\n",
            "validation\n",
            "validate : 1391 / 1480 * 100 = 93.98648648648648 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/094_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/095_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 97 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "96     97  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "92     93  93.986486\n",
            "93     94  93.986486\n",
            "94     95  94.121622\n",
            "95     96  93.986486\n",
            "96     97  94.324324\n",
            "validation\n",
            "validate : 1396 / 1480 * 100 = 94.32432432432432 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.1891891891892\n",
            "Best model Saved\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/095_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/096_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 98 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "96     97  100.000000\n",
            "97     98  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "92     93  93.986486\n",
            "93     94  93.986486\n",
            "94     95  94.121622\n",
            "95     96  93.986486\n",
            "96     97  94.324324\n",
            "97     98  94.391892\n",
            "validation\n",
            "validate : 1397 / 1480 * 100 = 94.39189189189189 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.32432432432432\n",
            "Best model Saved\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/096_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/097_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 99 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "96     97  100.000000\n",
            "97     98  100.000000\n",
            "98     99  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "92     93  93.986486\n",
            "93     94  93.986486\n",
            "94     95  94.121622\n",
            "95     96  93.986486\n",
            "96     97  94.324324\n",
            "97     98  94.391892\n",
            "98     99  94.121622\n",
            "validation\n",
            "validate : 1393 / 1480 * 100 = 94.12162162162161 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.39189189189189\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/097_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/098_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "Epoch 100 Complete\n",
            "call evaluation\n",
            "call print_evaluation_accuracy\n",
            "evaluation\n",
            "    epoch         acc\n",
            "0       1   15.037594\n",
            "1       2   28.947368\n",
            "2       3   44.736842\n",
            "3       4   55.263158\n",
            "4       5   77.443609\n",
            "5       6   81.578947\n",
            "6       7   98.872180\n",
            "7       8   92.105263\n",
            "8       9  100.000000\n",
            "9      10  100.000000\n",
            "10     11  100.000000\n",
            "11     12  100.000000\n",
            "12     13  100.000000\n",
            "13     14   98.496241\n",
            "14     15   99.624060\n",
            "15     16  100.000000\n",
            "16     17  100.000000\n",
            "17     18  100.000000\n",
            "18     19  100.000000\n",
            "19     20  100.000000\n",
            "20     21  100.000000\n",
            "21     22  100.000000\n",
            "22     23  100.000000\n",
            "23     24  100.000000\n",
            "24     25  100.000000\n",
            "25     26  100.000000\n",
            "26     27  100.000000\n",
            "27     28  100.000000\n",
            "28     29  100.000000\n",
            "29     30  100.000000\n",
            "30     31  100.000000\n",
            "31     32  100.000000\n",
            "32     33  100.000000\n",
            "33     34  100.000000\n",
            "34     35  100.000000\n",
            "35     36  100.000000\n",
            "36     37  100.000000\n",
            "37     38  100.000000\n",
            "38     39  100.000000\n",
            "39     40  100.000000\n",
            "40     41  100.000000\n",
            "41     42  100.000000\n",
            "42     43  100.000000\n",
            "43     44  100.000000\n",
            "44     45  100.000000\n",
            "45     46  100.000000\n",
            "46     47  100.000000\n",
            "47     48  100.000000\n",
            "48     49  100.000000\n",
            "49     50  100.000000\n",
            "50     51  100.000000\n",
            "51     52  100.000000\n",
            "52     53  100.000000\n",
            "53     54  100.000000\n",
            "54     55  100.000000\n",
            "55     56  100.000000\n",
            "56     57  100.000000\n",
            "57     58  100.000000\n",
            "58     59  100.000000\n",
            "59     60  100.000000\n",
            "60     61  100.000000\n",
            "61     62  100.000000\n",
            "62     63  100.000000\n",
            "63     64  100.000000\n",
            "64     65  100.000000\n",
            "65     66  100.000000\n",
            "66     67  100.000000\n",
            "67     68  100.000000\n",
            "68     69  100.000000\n",
            "69     70  100.000000\n",
            "70     71  100.000000\n",
            "71     72  100.000000\n",
            "72     73  100.000000\n",
            "73     74  100.000000\n",
            "74     75  100.000000\n",
            "75     76  100.000000\n",
            "76     77  100.000000\n",
            "77     78  100.000000\n",
            "78     79  100.000000\n",
            "79     80  100.000000\n",
            "80     81  100.000000\n",
            "81     82  100.000000\n",
            "82     83  100.000000\n",
            "83     84  100.000000\n",
            "84     85  100.000000\n",
            "85     86  100.000000\n",
            "86     87  100.000000\n",
            "87     88  100.000000\n",
            "88     89  100.000000\n",
            "89     90  100.000000\n",
            "90     91  100.000000\n",
            "91     92  100.000000\n",
            "92     93  100.000000\n",
            "93     94  100.000000\n",
            "94     95  100.000000\n",
            "95     96  100.000000\n",
            "96     97  100.000000\n",
            "97     98  100.000000\n",
            "98     99  100.000000\n",
            "99    100  100.000000\n",
            "evaluation\n",
            "evaluation : 266 / 266 * 100 = 100.0 \n",
            "call validate\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_validation_accuracy\n",
            "validation\n",
            "    epoch        acc\n",
            "0       1  14.797297\n",
            "1       2  27.905405\n",
            "2       3  39.932432\n",
            "3       4  52.297297\n",
            "4       5  73.986486\n",
            "5       6  76.824324\n",
            "6       7  88.918919\n",
            "7       8  84.594595\n",
            "8       9  90.608108\n",
            "9      10  92.229730\n",
            "10     11  91.891892\n",
            "11     12  92.770270\n",
            "12     13  89.594595\n",
            "13     14  89.729730\n",
            "14     15  90.135135\n",
            "15     16  93.513514\n",
            "16     17  91.418919\n",
            "17     18  93.378378\n",
            "18     19  93.310811\n",
            "19     20  93.445946\n",
            "20     21  93.445946\n",
            "21     22  93.378378\n",
            "22     23  93.243243\n",
            "23     24  93.445946\n",
            "24     25  93.445946\n",
            "25     26  93.445946\n",
            "26     27  93.378378\n",
            "27     28  93.378378\n",
            "28     29  93.378378\n",
            "29     30  93.445946\n",
            "30     31  93.175676\n",
            "31     32  92.837838\n",
            "32     33  93.445946\n",
            "33     34  93.243243\n",
            "34     35  93.445946\n",
            "35     36  93.378378\n",
            "36     37  93.310811\n",
            "37     38  93.243243\n",
            "38     39  93.310811\n",
            "39     40  93.310811\n",
            "40     41  93.581081\n",
            "41     42  93.581081\n",
            "42     43  93.310811\n",
            "43     44  93.310811\n",
            "44     45  93.445946\n",
            "45     46  93.581081\n",
            "46     47  93.445946\n",
            "47     48  93.378378\n",
            "48     49  93.513514\n",
            "49     50  93.243243\n",
            "50     51  93.378378\n",
            "51     52  93.310811\n",
            "52     53  93.175676\n",
            "53     54  93.513514\n",
            "54     55  93.581081\n",
            "55     56  93.581081\n",
            "56     57  93.378378\n",
            "57     58  93.445946\n",
            "58     59  93.513514\n",
            "59     60  93.716216\n",
            "60     61  93.648649\n",
            "61     62  93.445946\n",
            "62     63  93.310811\n",
            "63     64  93.378378\n",
            "64     65  93.378378\n",
            "65     66  93.445946\n",
            "66     67  92.297297\n",
            "67     68  93.513514\n",
            "68     69  93.108108\n",
            "69     70  93.783784\n",
            "70     71  93.918919\n",
            "71     72  93.986486\n",
            "72     73  93.851351\n",
            "73     74  93.986486\n",
            "74     75  93.918919\n",
            "75     76  93.986486\n",
            "76     77  93.986486\n",
            "77     78  94.189189\n",
            "78     79  93.716216\n",
            "79     80  93.513514\n",
            "80     81  93.310811\n",
            "81     82  93.378378\n",
            "82     83  93.243243\n",
            "83     84  93.310811\n",
            "84     85  93.445946\n",
            "85     86  93.513514\n",
            "86     87  93.243243\n",
            "87     88  93.108108\n",
            "88     89  93.175676\n",
            "89     90  93.445946\n",
            "90     91  93.310811\n",
            "91     92  93.851351\n",
            "92     93  93.986486\n",
            "93     94  93.986486\n",
            "94     95  94.121622\n",
            "95     96  93.986486\n",
            "96     97  94.324324\n",
            "97     98  94.391892\n",
            "98     99  94.121622\n",
            "99    100  94.121622\n",
            "validation\n",
            "validate : 1393 / 1480 * 100 = 94.12162162162161 \n",
            "call save_best_model\n",
            "output_dir : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "best_model_accuracy : 94.39189189189189\n",
            "call save_params\n",
            "call create_path_if_not_exists\n",
            "dir_path : /content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2\n",
            "['/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/098_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/099_persiannews_0.02_0.1_0.2.pth', '/content/drive/MyDrive/NLP/save/ec_gan|persiannews|0.02|0.1|0.2/best_model.pth']\n",
            "call load_best_model\n",
            "call test\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "call print_test_accuracy\n",
            "test\n",
            "         acc\n",
            "0  94.038929\n",
            "test\n",
            "test : 1546 / 1644 * 100 = 94.0389294403893 \n",
            "      train data evaluation validation data evaluation\n",
            "0   [1, 15.037593984962406]    [1, 14.797297297297296]\n",
            "1   [2, 28.947368421052634]    [2, 27.905405405405403]\n",
            "2    [3, 44.73684210526316]     [3, 39.93243243243243]\n",
            "3    [4, 55.26315789473685]      [4, 52.2972972972973]\n",
            "4    [5, 77.44360902255639]     [5, 73.98648648648648]\n",
            "5    [6, 81.57894736842105]     [6, 76.82432432432432]\n",
            "6    [7, 98.87218045112782]     [7, 88.91891891891892]\n",
            "7    [8, 92.10526315789474]      [8, 84.5945945945946]\n",
            "8                [9, 100.0]     [9, 90.60810810810811]\n",
            "9               [10, 100.0]    [10, 92.22972972972973]\n",
            "10              [11, 100.0]     [11, 91.8918918918919]\n",
            "11              [12, 100.0]    [12, 92.77027027027027]\n",
            "12              [13, 100.0]     [13, 89.5945945945946]\n",
            "13  [14, 98.49624060150376]    [14, 89.72972972972974]\n",
            "14  [15, 99.62406015037594]    [15, 90.13513513513513]\n",
            "15              [16, 100.0]    [16, 93.51351351351352]\n",
            "16              [17, 100.0]    [17, 91.41891891891892]\n",
            "17              [18, 100.0]    [18, 93.37837837837839]\n",
            "18              [19, 100.0]     [19, 93.3108108108108]\n",
            "19              [20, 100.0]    [20, 93.44594594594594]\n",
            "20              [21, 100.0]    [21, 93.44594594594594]\n",
            "21              [22, 100.0]    [22, 93.37837837837839]\n",
            "22              [23, 100.0]    [23, 93.24324324324324]\n",
            "23              [24, 100.0]    [24, 93.44594594594594]\n",
            "24              [25, 100.0]    [25, 93.44594594594594]\n",
            "25              [26, 100.0]    [26, 93.44594594594594]\n",
            "26              [27, 100.0]    [27, 93.37837837837839]\n",
            "27              [28, 100.0]    [28, 93.37837837837839]\n",
            "28              [29, 100.0]    [29, 93.37837837837839]\n",
            "29              [30, 100.0]    [30, 93.44594594594594]\n",
            "30              [31, 100.0]    [31, 93.17567567567568]\n",
            "31              [32, 100.0]    [32, 92.83783783783784]\n",
            "32              [33, 100.0]    [33, 93.44594594594594]\n",
            "33              [34, 100.0]    [34, 93.24324324324324]\n",
            "34              [35, 100.0]    [35, 93.44594594594594]\n",
            "35              [36, 100.0]    [36, 93.37837837837839]\n",
            "36              [37, 100.0]     [37, 93.3108108108108]\n",
            "37              [38, 100.0]    [38, 93.24324324324324]\n",
            "38              [39, 100.0]     [39, 93.3108108108108]\n",
            "39              [40, 100.0]     [40, 93.3108108108108]\n",
            "40              [41, 100.0]    [41, 93.58108108108108]\n",
            "41              [42, 100.0]    [42, 93.58108108108108]\n",
            "42              [43, 100.0]     [43, 93.3108108108108]\n",
            "43              [44, 100.0]     [44, 93.3108108108108]\n",
            "44              [45, 100.0]    [45, 93.44594594594594]\n",
            "45              [46, 100.0]    [46, 93.58108108108108]\n",
            "46              [47, 100.0]    [47, 93.44594594594594]\n",
            "47              [48, 100.0]    [48, 93.37837837837839]\n",
            "48              [49, 100.0]    [49, 93.51351351351352]\n",
            "49              [50, 100.0]    [50, 93.24324324324324]\n",
            "50              [51, 100.0]    [51, 93.37837837837839]\n",
            "51              [52, 100.0]     [52, 93.3108108108108]\n",
            "52              [53, 100.0]    [53, 93.17567567567568]\n",
            "53              [54, 100.0]    [54, 93.51351351351352]\n",
            "54              [55, 100.0]    [55, 93.58108108108108]\n",
            "55              [56, 100.0]    [56, 93.58108108108108]\n",
            "56              [57, 100.0]    [57, 93.37837837837839]\n",
            "57              [58, 100.0]    [58, 93.44594594594594]\n",
            "58              [59, 100.0]    [59, 93.51351351351352]\n",
            "59              [60, 100.0]    [60, 93.71621621621622]\n",
            "60              [61, 100.0]    [61, 93.64864864864865]\n",
            "61              [62, 100.0]    [62, 93.44594594594594]\n",
            "62              [63, 100.0]     [63, 93.3108108108108]\n",
            "63              [64, 100.0]    [64, 93.37837837837839]\n",
            "64              [65, 100.0]    [65, 93.37837837837839]\n",
            "65              [66, 100.0]    [66, 93.44594594594594]\n",
            "66              [67, 100.0]    [67, 92.29729729729729]\n",
            "67              [68, 100.0]    [68, 93.51351351351352]\n",
            "68              [69, 100.0]    [69, 93.10810810810811]\n",
            "69              [70, 100.0]    [70, 93.78378378378378]\n",
            "70              [71, 100.0]    [71, 93.91891891891892]\n",
            "71              [72, 100.0]    [72, 93.98648648648648]\n",
            "72              [73, 100.0]    [73, 93.85135135135135]\n",
            "73              [74, 100.0]    [74, 93.98648648648648]\n",
            "74              [75, 100.0]    [75, 93.91891891891892]\n",
            "75              [76, 100.0]    [76, 93.98648648648648]\n",
            "76              [77, 100.0]    [77, 93.98648648648648]\n",
            "77              [78, 100.0]     [78, 94.1891891891892]\n",
            "78              [79, 100.0]    [79, 93.71621621621622]\n",
            "79              [80, 100.0]    [80, 93.51351351351352]\n",
            "80              [81, 100.0]     [81, 93.3108108108108]\n",
            "81              [82, 100.0]    [82, 93.37837837837839]\n",
            "82              [83, 100.0]    [83, 93.24324324324324]\n",
            "83              [84, 100.0]     [84, 93.3108108108108]\n",
            "84              [85, 100.0]    [85, 93.44594594594594]\n",
            "85              [86, 100.0]    [86, 93.51351351351352]\n",
            "86              [87, 100.0]    [87, 93.24324324324324]\n",
            "87              [88, 100.0]    [88, 93.10810810810811]\n",
            "88              [89, 100.0]    [89, 93.17567567567568]\n",
            "89              [90, 100.0]    [90, 93.44594594594594]\n",
            "90              [91, 100.0]     [91, 93.3108108108108]\n",
            "91              [92, 100.0]    [92, 93.85135135135135]\n",
            "92              [93, 100.0]    [93, 93.98648648648648]\n",
            "93              [94, 100.0]    [94, 93.98648648648648]\n",
            "94              [95, 100.0]    [95, 94.12162162162161]\n",
            "95              [96, 100.0]    [96, 93.98648648648648]\n",
            "96              [97, 100.0]    [97, 94.32432432432432]\n",
            "97              [98, 100.0]    [98, 94.39189189189189]\n",
            "98              [99, 100.0]    [99, 94.12162162162161]\n",
            "99             [100, 100.0]   [100, 94.12162162162161]\n",
            "   test data evaluation\n",
            "0             94.038929\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d digikalamag -p 0.01\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d persiannews -p 0.02 -w 0.1 -t 0.2 -e 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# a = [\"1\",\"2\",\"3\"]\n",
        "# b = [\"4\",\"5\",\"6\"]\n",
        "\n",
        "# df = pd.DataFrame([x for x in zip(a,b)],columns=['train data evaluation','validation data evaluation'])\n",
        "# df"
      ],
      "metadata": {
        "id": "Jwsrq0dgPxmX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# a = [\"1\",\"2\",\"3\"]\n",
        "# b = [\"4\",\"5\",\"6\"]\n",
        "# z = zip(a,b)\n",
        "# lst = []\n",
        "# for i in z:\n",
        "#   print(i)\n",
        "#   lst.append(i)\n",
        "\n",
        "# df = pd.DataFrame(lst,columns=[\"a\",\"b\"])\n",
        "# df"
      ],
      "metadata": {
        "id": "HljqZDToGyDW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tTk2OUDsOiKm"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}