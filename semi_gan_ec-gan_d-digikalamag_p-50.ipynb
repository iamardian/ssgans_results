{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/semi_gan_ec-gan_d-digikalamag_p-50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "03869b13-ac42-4e0f-ed8f-d27c3bf907c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 55 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 40.625\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 32.291666666666664\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 34.375\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 34.375\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 33.333333333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.482142857142854\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 32.421875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 30.90277777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 30.9375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 55 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 56 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 21.875\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 26.041666666666668\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 26.5625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 30.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 29.6875\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 30.803571428571427\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 30.859375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 32.291666666666664\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 32.8125\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 56 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 57 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 34.375\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 37.5\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 38.125\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 36.458333333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 35.714285714285715\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 33.984375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 32.638888888888886\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 32.1875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 57 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 58 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 23.4375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 32.03125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 32.5\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 31.770833333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 31.696428571428573\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 32.8125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 32.291666666666664\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 32.1875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 58 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 59 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 29.6875\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 32.291666666666664\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 33.59375\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 32.5\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 29.6875\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.910714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 30.078125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 29.166666666666668\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 28.75\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 59 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 60 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 35.9375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 30.208333333333332\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 30.46875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 30.0\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 30.208333333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 30.357142857142858\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 30.078125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 29.86111111111111\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 29.6875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 60 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 61 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 50.0\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 35.9375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 35.416666666666664\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.15625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 33.125\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 32.8125\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.482142857142854\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 31.640625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 30.90277777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 32.1875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 61 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 62 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 53.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 46.875\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 42.708333333333336\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 41.40625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 40.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 39.583333333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 37.94642857142857\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 36.328125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 34.72222222222222\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 34.0625\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 62 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 63 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 29.6875\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 32.291666666666664\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.90625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 31.25\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 31.770833333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.92857142857143\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 33.59375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 34.02777777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 32.5\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 63 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 64 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 37.5\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 35.9375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 36.458333333333336\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 32.8125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 31.875\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 33.333333333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.482142857142854\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 33.59375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 33.68055555555556\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 33.125\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 64 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 65 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 34.375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 37.5\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.15625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 33.75\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 32.8125\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 32.589285714285715\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 36.328125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 36.111111111111114\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 35.9375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 65 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 66 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 37.5\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 40.625\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 34.375\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 31.25\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 31.25\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 31.770833333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.035714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 31.640625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 30.90277777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 31.875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 66 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 67 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 26.5625\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 25.0\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 33.125\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 34.375\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.035714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 34.375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 32.986111111111114\n",
            "  Batch    90  of    108.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 33.4375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 67 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 68 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 25.0\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 32.8125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 32.5\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 32.291666666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 31.25\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 32.8125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 33.333333333333336\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 33.125\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 68 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 69 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 31.25\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 27.34375\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 30.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 32.291666666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 32.589285714285715\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 31.640625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 30.90277777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 31.875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 69 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 70 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 31.25\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 34.375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 31.25\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 31.25\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 31.25\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 31.770833333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 34.375\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 36.71875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 35.06944444444444\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 34.375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 70 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 71 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 31.25\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 34.375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 33.333333333333336\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.15625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 32.5\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 32.8125\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 32.142857142857146\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 31.640625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 30.90277777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 32.8125\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 71 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 72 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 31.25\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 31.25\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 32.03125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 30.0\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 28.125\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.017857142857142\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 28.90625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 31.25\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 32.1875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 72 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 73 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 35.9375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 31.25\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 30.46875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 30.0\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 29.6875\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.910714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 29.6875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 28.819444444444443\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 29.6875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 73 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 74 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 37.5\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 36.458333333333336\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 32.03125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 31.875\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 32.291666666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 31.696428571428573\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 33.984375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 34.375\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 35.9375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 74 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 75 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 43.75\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 43.75\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 34.375\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 37.5\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 35.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 34.895833333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.035714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 32.421875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 32.638888888888886\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 32.1875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 75 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 76 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 29.6875\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 28.125\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 30.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 32.291666666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 31.25\n",
            "  Batch    70  of    108.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 32.421875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 34.02777777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 33.4375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 76 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 77 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 25.0\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 28.125\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 25.78125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 25.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 27.604166666666668\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 30.357142857142858\n",
            "  Batch    70  of    108.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 28.90625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 29.51388888888889\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 28.75\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 77 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 35.9375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 38.541666666666664\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.9375\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 36.875\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 38.020833333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 35.714285714285715\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 36.328125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 34.72222222222222\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 33.4375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 78 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 40.625\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 35.9375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 37.5\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.9375\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 36.25\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 35.416666666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.482142857142854\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 32.8125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 31.944444444444443\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 30.9375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 79 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 40.625\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 39.0625\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 37.5\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.15625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 34.375\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 33.854166666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 31.696428571428573\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 32.8125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 33.68055555555556\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 34.6875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 80 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 25.0\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 25.0\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 29.375\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 28.645833333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 28.571428571428573\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 28.90625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 29.51388888888889\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 28.4375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 81 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 40.625\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 37.5\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 35.416666666666664\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.15625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 35.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 34.895833333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.035714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 33.984375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 34.02777777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 35.0\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 82 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 31.25\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.90625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 30.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 29.6875\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.910714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 30.859375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 29.86111111111111\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 29.375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 83 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 30.208333333333332\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.15625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 35.0\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 33.854166666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 32.589285714285715\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 31.25\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 31.59722222222222\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 30.9375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 84 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 40.625\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 31.25\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 31.25\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 29.6875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 30.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 30.208333333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.464285714285715\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 29.296875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 30.90277777777778\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 30.9375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 85 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 31.25\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 34.375\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 32.8125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 37.5\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 38.541666666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 36.160714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 35.9375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 36.458333333333336\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 35.9375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 86 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 43.75\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 39.0625\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 38.541666666666664\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.9375\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 35.0\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 32.291666666666664\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 32.142857142857146\n",
            "  Batch    70  of    108.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 31.640625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 33.333333333333336\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 34.375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 87 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 26.5625\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 26.875\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 26.5625\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.017857142857142\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 28.515625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 29.51388888888889\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 30.0\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 88 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 29.6875\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 30.208333333333332\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 30.46875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 30.0\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 30.208333333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.92857142857143\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 33.59375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 31.59722222222222\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 30.625\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 89 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 40.625\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 37.5\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 36.71875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 37.5\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 36.458333333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 36.160714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 36.71875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 35.763888888888886\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 35.3125\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 90 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 31.25\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 26.5625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 28.125\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 27.083333333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 27.678571428571427\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 26.171875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 27.083333333333332\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 27.1875\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 91 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 31.25\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 34.375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 28.125\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 30.46875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 29.375\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 30.729166666666668\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 32.589285714285715\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 31.25\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 32.638888888888886\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 33.125\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 92 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 37.5\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 37.5\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 36.458333333333336\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 36.71875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 36.25\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 36.458333333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 35.267857142857146\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 37.109375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 36.458333333333336\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 35.625\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 93 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 23.4375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 29.166666666666668\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 30.46875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 32.5\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 33.333333333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.910714285714285\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 28.125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 28.47222222222222\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 28.75\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 94 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "93     94  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 43.75\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 43.75\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 43.75\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 40.625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 40.0\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 38.020833333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 36.607142857142854\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 34.765625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 35.763888888888886\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 33.75\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 95 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "93     94  32.511737\n",
            "94     95  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 28.125\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 26.041666666666668\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 29.6875\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 30.0\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 30.208333333333332\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 27.678571428571427\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 28.125\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 28.125\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 29.0625\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 96 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "93     94  32.511737\n",
            "94     95  32.511737\n",
            "95     96  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 34.375\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 31.25\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 31.25\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 32.03125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 36.25\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 37.5\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 37.5\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 36.71875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 35.416666666666664\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 35.0\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 97 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "93     94  32.511737\n",
            "94     95  32.511737\n",
            "95     96  32.511737\n",
            "96     97  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 40.625\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 34.375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 32.291666666666664\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 32.8125\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 32.5\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 33.333333333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.482142857142854\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 34.765625\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 33.333333333333336\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 33.4375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 98 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "93     94  32.511737\n",
            "94     95  32.511737\n",
            "95     96  32.511737\n",
            "96     97  32.511737\n",
            "97     98  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 34.375\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 33.333333333333336\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 33.59375\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 35.625\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 34.895833333333336\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 33.482142857142854\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 33.59375\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 33.68055555555556\n",
            "  Batch    90  of    108.    Elapsed: 0:02:03.\n",
            "(100) train_accuracy : 33.75\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 99 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "93     94  32.511737\n",
            "94     95  32.511737\n",
            "95     96  32.511737\n",
            "96     97  32.511737\n",
            "97     98  32.511737\n",
            "98     99  23.122066\n",
            "197 / 852 * 100 = 23.122065727699532 \n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of    108.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 25.0\n",
            "  Batch    20  of    108.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 29.166666666666668\n",
            "  Batch    30  of    108.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.90625\n",
            "  Batch    40  of    108.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 28.125\n",
            "  Batch    50  of    108.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 27.604166666666668\n",
            "  Batch    60  of    108.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.017857142857142\n",
            "  Batch    70  of    108.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 29.6875\n",
            "  Batch    80  of    108.    Elapsed: 0:01:49.\n",
            "(90) train_accuracy : 29.86111111111111\n",
            "  Batch    90  of    108.    Elapsed: 0:02:02.\n",
            "(100) train_accuracy : 30.9375\n",
            "  Batch   100  of    108.    Elapsed: 0:02:16.\n",
            "Epoch 100 Complete\n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "93     94  32.511737\n",
            "94     95  32.511737\n",
            "95     96  32.511737\n",
            "96     97  32.511737\n",
            "97     98  32.511737\n",
            "98     99  23.122066\n",
            "99    100  32.511737\n",
            "277 / 852 * 100 = 32.51173708920187 \n",
            "    epoch        acc\n",
            "0       1  86.619718\n",
            "1       2  79.812207\n",
            "2       3  82.042254\n",
            "3       4  67.488263\n",
            "4       5  38.497653\n",
            "5       6  23.122066\n",
            "6       7  32.511737\n",
            "7       8  32.511737\n",
            "8       9  32.511737\n",
            "9      10  32.511737\n",
            "10     11  32.511737\n",
            "11     12  32.511737\n",
            "12     13  32.511737\n",
            "13     14  32.511737\n",
            "14     15  32.511737\n",
            "15     16  32.511737\n",
            "16     17  32.511737\n",
            "17     18  32.511737\n",
            "18     19  32.511737\n",
            "19     20  32.511737\n",
            "20     21  32.511737\n",
            "21     22  32.511737\n",
            "22     23  32.511737\n",
            "23     24  32.511737\n",
            "24     25  32.511737\n",
            "25     26  32.511737\n",
            "26     27  32.511737\n",
            "27     28  32.511737\n",
            "28     29  32.511737\n",
            "29     30  32.511737\n",
            "30     31  32.511737\n",
            "31     32  32.511737\n",
            "32     33  32.511737\n",
            "33     34  32.511737\n",
            "34     35  32.511737\n",
            "35     36  32.511737\n",
            "36     37  32.511737\n",
            "37     38  32.511737\n",
            "38     39  32.511737\n",
            "39     40  32.511737\n",
            "40     41  32.511737\n",
            "41     42  32.511737\n",
            "42     43  32.511737\n",
            "43     44  32.511737\n",
            "44     45  32.511737\n",
            "45     46  32.511737\n",
            "46     47  32.511737\n",
            "47     48  32.511737\n",
            "48     49  32.511737\n",
            "49     50  32.511737\n",
            "50     51  32.511737\n",
            "51     52  32.511737\n",
            "52     53  32.511737\n",
            "53     54  32.511737\n",
            "54     55  32.511737\n",
            "55     56  32.511737\n",
            "56     57  32.511737\n",
            "57     58  32.511737\n",
            "58     59  32.511737\n",
            "59     60  32.511737\n",
            "60     61  32.511737\n",
            "61     62  32.511737\n",
            "62     63  32.511737\n",
            "63     64  32.511737\n",
            "64     65  32.511737\n",
            "65     66  32.511737\n",
            "66     67  32.511737\n",
            "67     68  32.511737\n",
            "68     69  32.511737\n",
            "69     70  32.511737\n",
            "70     71  32.511737\n",
            "71     72  32.511737\n",
            "72     73  32.511737\n",
            "73     74  32.511737\n",
            "74     75  32.511737\n",
            "75     76  32.511737\n",
            "76     77  32.511737\n",
            "77     78  32.511737\n",
            "78     79  32.511737\n",
            "79     80  32.511737\n",
            "80     81  32.511737\n",
            "81     82  32.511737\n",
            "82     83  32.511737\n",
            "83     84  32.511737\n",
            "84     85  32.511737\n",
            "85     86  32.511737\n",
            "86     87  32.511737\n",
            "87     88  32.511737\n",
            "88     89  32.511737\n",
            "89     90  32.511737\n",
            "90     91  32.511737\n",
            "91     92  32.511737\n",
            "92     93  32.511737\n",
            "93     94  32.511737\n",
            "94     95  32.511737\n",
            "95     96  32.511737\n",
            "96     97  32.511737\n",
            "97     98  32.511737\n",
            "98     99  23.122066\n",
            "99    100  32.511737\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d digikalamag -p 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}