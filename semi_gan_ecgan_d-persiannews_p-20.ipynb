{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamardian/ssgans_results/blob/main/semi_gan_ecgan_d-persiannews_p-20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVzghhQznhMw",
        "outputId": "9baa3b72-fe13-424c-8792-3799b5856eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "======== Epoch 52 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 56.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 56.25\n",
            "  Batch    20  of     84.    Elapsed: 0:00:28.\n",
            "(30) train_accuracy : 48.958333333333336\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 50.0\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 50.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 53.645833333333336\n",
            "  Batch    60  of     84.    Elapsed: 0:01:23.\n",
            "(70) train_accuracy : 52.67857142857143\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 50.78125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 52 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "571 / 1644 * 100 = 34.7323600973236 \n",
            "\n",
            "======== Epoch 53 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 31.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 37.5\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 34.375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 35.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 33.333333333333336\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 31.696428571428573\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 32.421875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 53 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "456 / 1644 * 100 = 27.73722627737226 \n",
            "\n",
            "======== Epoch 54 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 14.0625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 14.583333333333334\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.1875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 20.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 21.354166666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 20.089285714285715\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 18.75\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 54 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "386 / 1644 * 100 = 23.479318734793186 \n",
            "\n",
            "======== Epoch 55 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 25.0\n",
            "  Batch    20  of     84.    Elapsed: 0:00:28.\n",
            "(30) train_accuracy : 21.875\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 23.4375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 21.875\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 21.875\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 23.214285714285715\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 22.265625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 55 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "385 / 1644 * 100 = 23.418491484184916 \n",
            "\n",
            "======== Epoch 56 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 28.125\n",
            "  Batch    20  of     84.    Elapsed: 0:00:28.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 25.0\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 26.041666666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 25.892857142857142\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 26.171875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 56 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "386 / 1644 * 100 = 23.479318734793186 \n",
            "\n",
            "======== Epoch 57 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 28.125\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 29.6875\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 29.375\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 29.166666666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 28.125\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 26.171875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 57 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "331 / 1644 * 100 = 20.1338199513382 \n",
            "\n",
            "======== Epoch 58 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 23.4375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 22.916666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 24.21875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 25.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 26.5625\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 26.339285714285715\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 28.125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 58 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "394 / 1644 * 100 = 23.96593673965937 \n",
            "\n",
            "======== Epoch 59 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 23.4375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 28.125\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 25.78125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 27.5\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 29.6875\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.017857142857142\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 29.296875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 59 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "436 / 1644 * 100 = 26.520681265206814 \n",
            "\n",
            "======== Epoch 60 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 21.875\n",
            "  Batch    20  of     84.    Elapsed: 0:00:28.\n",
            "(30) train_accuracy : 26.041666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 27.34375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 25.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 27.083333333333332\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 26.339285714285715\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 25.78125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 60 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "447 / 1644 * 100 = 27.189781021897808 \n",
            "\n",
            "======== Epoch 61 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 26.5625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 23.4375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 26.25\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 26.5625\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 25.446428571428573\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 25.0\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 61 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "363 / 1644 * 100 = 22.08029197080292 \n",
            "\n",
            "======== Epoch 62 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of     84.    Elapsed: 0:00:28.\n",
            "(30) train_accuracy : 19.791666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 19.53125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 16.875\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 16.666666666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 17.410714285714285\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 17.1875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 62 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "362 / 1644 * 100 = 22.019464720194648 \n",
            "\n",
            "======== Epoch 63 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 23.4375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 20.833333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 23.4375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 26.25\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 27.083333333333332\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 26.339285714285715\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 25.0\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 63 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "431 / 1644 * 100 = 26.216545012165447 \n",
            "\n",
            "======== Epoch 64 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of     84.    Elapsed: 0:00:28.\n",
            "(30) train_accuracy : 15.625\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 22.65625\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 24.375\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 24.479166666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:23.\n",
            "(70) train_accuracy : 25.892857142857142\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 25.0\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 64 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "430 / 1644 * 100 = 26.155717761557177 \n",
            "\n",
            "======== Epoch 65 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 28.125\n",
            "  Batch    20  of     84.    Elapsed: 0:00:28.\n",
            "(30) train_accuracy : 29.166666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 30.46875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 30.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 29.166666666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 27.678571428571427\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 26.953125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 65 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "360 / 1644 * 100 = 21.897810218978105 \n",
            "\n",
            "======== Epoch 66 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 31.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 32.8125\n",
            "  Batch    20  of     84.    Elapsed: 0:00:28.\n",
            "(30) train_accuracy : 31.25\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 35.9375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 38.125\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 38.541666666666664\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 38.839285714285715\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 37.5\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 66 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "430 / 1644 * 100 = 26.155717761557177 \n",
            "\n",
            "======== Epoch 67 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 21.875\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 22.916666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 26.5625\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 25.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 26.5625\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 28.125\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 29.296875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 67 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "434 / 1644 * 100 = 26.399026763990268 \n",
            "\n",
            "======== Epoch 68 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 31.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 31.25\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 30.208333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 26.5625\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 26.875\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 27.604166666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 28.125\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 28.515625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 68 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "447 / 1644 * 100 = 27.189781021897808 \n",
            "\n",
            "======== Epoch 69 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 26.5625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 30.46875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 30.0\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 30.208333333333332\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 32.142857142857146\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 33.984375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 69 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "448 / 1644 * 100 = 27.25060827250608 \n",
            "\n",
            "======== Epoch 70 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 26.5625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 22.916666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 21.09375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 21.875\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 23.958333333333332\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 23.660714285714285\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 22.65625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 70 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "443 / 1644 * 100 = 26.946472019464718 \n",
            "\n",
            "======== Epoch 71 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 25.0\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 26.5625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 27.083333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 28.125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 32.5\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 31.25\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 29.910714285714285\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 29.6875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 71 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "446 / 1644 * 100 = 27.128953771289538 \n",
            "\n",
            "======== Epoch 72 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 40.625\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 37.5\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 30.208333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 25.78125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 24.375\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 23.4375\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 22.321428571428573\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 22.265625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 72 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "301 / 1644 * 100 = 18.309002433090026 \n",
            "\n",
            "======== Epoch 73 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 23.4375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 19.791666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 18.75\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 20.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 21.354166666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 20.982142857142858\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 20.703125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 73 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "165 / 1644 * 100 = 10.036496350364963 \n",
            "\n",
            "======== Epoch 74 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 6.25\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 5.208333333333333\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 5.46875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 5.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 9.375\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 11.160714285714286\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 10.9375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 74 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "291 / 1644 * 100 = 17.7007299270073 \n",
            "\n",
            "======== Epoch 75 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 20.3125\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 21.875\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 21.09375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 20.0\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 18.229166666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 16.517857142857142\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 15.625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 75 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "138 / 1644 * 100 = 8.394160583941606 \n",
            "\n",
            "======== Epoch 76 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 6.25\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 6.25\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 7.03125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 6.25\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 6.770833333333333\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 6.696428571428571\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 6.25\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 76 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "138 / 1644 * 100 = 8.394160583941606 \n",
            "\n",
            "======== Epoch 77 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 4.6875\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 3.125\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 5.46875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 6.25\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 7.8125\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 6.696428571428571\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 7.8125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 77 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "138 / 1644 * 100 = 8.394160583941606 \n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 3.125\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 4.6875\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 6.25\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 5.46875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 5.0\n",
            "  Batch    50  of     84.    Elapsed: 0:01:09.\n",
            "(60) train_accuracy : 4.6875\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 5.803571428571429\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 6.640625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:50.\n",
            "Epoch 78 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "138 / 1644 * 100 = 8.394160583941606 \n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 3.125\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 4.6875\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 4.166666666666667\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 4.6875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 5.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 7.291666666666667\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 7.589285714285714\n",
            "  Batch    70  of     84.    Elapsed: 0:01:36.\n",
            "(80) train_accuracy : 8.203125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 79 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "138 / 1644 * 100 = 8.394160583941606 \n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 10.9375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 7.291666666666667\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 7.03125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 8.75\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 8.854166666666666\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 10.267857142857142\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 10.15625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 80 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 10.9375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 10.416666666666666\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 10.9375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 11.875\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 13.541666666666666\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 15.178571428571429\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.84375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 81 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 18.75\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 19.53125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 16.25\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.145833333333332\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 15.178571428571429\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 15.234375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 82 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 17.5\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.666666666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 16.964285714285715\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.40625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 83 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "386 / 1644 * 100 = 23.479318734793186 \n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 23.4375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 21.875\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 23.4375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 25.0\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 24.479166666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 25.0\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 24.609375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 84 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "322 / 1644 * 100 = 19.586374695863746 \n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 19.791666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 20.3125\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 20.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 20.3125\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 20.535714285714285\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 21.484375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 85 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.104166666666666\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 14.285714285714286\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 15.234375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 86 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "256 / 1644 * 100 = 15.571776155717762 \n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 3.125\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 6.25\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 12.5\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.0625\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.75\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 13.020833333333334\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 13.839285714285714\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 13.671875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 87 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "227 / 1644 * 100 = 13.80778588807786 \n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 10.9375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 10.416666666666666\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.125\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 13.541666666666666\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 14.732142857142858\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 15.234375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 88 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 15.625\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 15.625\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.0\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 15.625\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 15.178571428571429\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 14.0625\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 89 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "217 / 1644 * 100 = 13.199513381995134 \n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 6.25\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 6.25\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 9.375\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 8.59375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 8.125\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 10.416666666666666\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 10.267857142857142\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 12.109375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 90 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 10.9375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 8.333333333333334\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 11.71875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 12.5\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 10.9375\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 10.714285714285714\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 11.328125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 91 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "244 / 1644 * 100 = 14.841849148418493 \n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 9.375\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 10.9375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 11.458333333333334\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 10.9375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 11.25\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 10.416666666666666\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 9.375\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 10.9375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 92 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "209 / 1644 * 100 = 12.712895377128955 \n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 10.9375\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 12.5\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 13.125\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 12.5\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 12.5\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 11.71875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 93 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "309 / 1644 * 100 = 18.795620437956202 \n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 23.958333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 25.0\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 22.5\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 21.875\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 21.428571428571427\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 21.09375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 94 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "93     94  18.126521\n",
            "298 / 1644 * 100 = 18.126520681265205 \n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 21.875\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 18.75\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 14.583333333333334\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:55.\n",
            "(50) train_accuracy : 18.125\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.666666666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 17.410714285714285\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 17.578125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 95 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "93     94  18.126521\n",
            "94     95  15.997567\n",
            "263 / 1644 * 100 = 15.997566909975669 \n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 17.708333333333332\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 17.1875\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 16.875\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 18.229166666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 17.410714285714285\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.796875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 96 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "93     94  18.126521\n",
            "94     95  15.997567\n",
            "95     96  17.092457\n",
            "281 / 1644 * 100 = 17.092457420924575 \n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 17.1875\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 19.791666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 22.65625\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 21.875\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 20.3125\n",
            "  Batch    60  of     84.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 20.089285714285715\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 20.3125\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 97 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "93     94  18.126521\n",
            "94     95  15.997567\n",
            "95     96  17.092457\n",
            "96     97  15.267640\n",
            "251 / 1644 * 100 = 15.2676399026764 \n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 12.5\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 16.666666666666668\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 14.84375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 15.625\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 16.666666666666668\n",
            "  Batch    60  of     84.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 16.071428571428573\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 16.796875\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 98 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "93     94  18.126521\n",
            "94     95  15.997567\n",
            "95     96  17.092457\n",
            "96     97  15.267640\n",
            "97     98  18.126521\n",
            "298 / 1644 * 100 = 18.126520681265205 \n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 15.625\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 15.625\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 18.75\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 21.09375\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 19.375\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 18.75\n",
            "  Batch    60  of     84.    Elapsed: 0:01:22.\n",
            "(70) train_accuracy : 18.75\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 18.75\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 99 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "93     94  18.126521\n",
            "94     95  15.997567\n",
            "95     96  17.092457\n",
            "96     97  15.267640\n",
            "97     98  18.126521\n",
            "98     99  18.309002\n",
            "301 / 1644 * 100 = 18.309002433090026 \n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "./semi_supervised_learning_with_gan/ecgan_parsbert.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(logits)\n",
            "(10) train_accuracy : 18.75\n",
            "  Batch    10  of     84.    Elapsed: 0:00:14.\n",
            "(20) train_accuracy : 12.5\n",
            "  Batch    20  of     84.    Elapsed: 0:00:27.\n",
            "(30) train_accuracy : 13.541666666666666\n",
            "  Batch    30  of     84.    Elapsed: 0:00:41.\n",
            "(40) train_accuracy : 16.40625\n",
            "  Batch    40  of     84.    Elapsed: 0:00:54.\n",
            "(50) train_accuracy : 17.5\n",
            "  Batch    50  of     84.    Elapsed: 0:01:08.\n",
            "(60) train_accuracy : 17.708333333333332\n",
            "  Batch    60  of     84.    Elapsed: 0:01:21.\n",
            "(70) train_accuracy : 17.857142857142858\n",
            "  Batch    70  of     84.    Elapsed: 0:01:35.\n",
            "(80) train_accuracy : 18.359375\n",
            "  Batch    80  of     84.    Elapsed: 0:01:49.\n",
            "Epoch 100 Complete\n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "93     94  18.126521\n",
            "94     95  15.997567\n",
            "95     96  17.092457\n",
            "96     97  15.267640\n",
            "97     98  18.126521\n",
            "98     99  18.309002\n",
            "99    100  17.274939\n",
            "284 / 1644 * 100 = 17.27493917274939 \n",
            "    epoch        acc\n",
            "0       1  94.647202\n",
            "1       2  94.464720\n",
            "2       3  91.666667\n",
            "3       4  92.518248\n",
            "4       5  90.875912\n",
            "5       6  91.909976\n",
            "6       7  94.160584\n",
            "7       8  93.978102\n",
            "8       9  94.282238\n",
            "9      10  86.618005\n",
            "10     11  90.936740\n",
            "11     12  90.450122\n",
            "12     13  92.761557\n",
            "13     14  93.430657\n",
            "14     15  93.248175\n",
            "15     16  93.552311\n",
            "16     17  93.673966\n",
            "17     18  93.795620\n",
            "18     19  93.552311\n",
            "19     20  94.343066\n",
            "20     21  93.004866\n",
            "21     22  91.909976\n",
            "22     23  93.552311\n",
            "23     24  74.817518\n",
            "24     25  89.902676\n",
            "25     26  85.340633\n",
            "26     27  72.323601\n",
            "27     28  75.608273\n",
            "28     29  72.262774\n",
            "29     30  72.445255\n",
            "30     31  79.197080\n",
            "31     32  79.987835\n",
            "32     33  90.754258\n",
            "33     34  90.145985\n",
            "34     35  90.875912\n",
            "35     36  86.982968\n",
            "36     37  90.571776\n",
            "37     38  86.922141\n",
            "38     39  91.484185\n",
            "39     40  89.294404\n",
            "40     41  88.929440\n",
            "41     42  77.250608\n",
            "42     43  74.817518\n",
            "43     44  75.060827\n",
            "44     45  64.781022\n",
            "45     46  61.496350\n",
            "46     47  58.941606\n",
            "47     48  62.165450\n",
            "48     49  68.978102\n",
            "49     50  59.610706\n",
            "50     51  51.520681\n",
            "51     52  34.732360\n",
            "52     53  27.737226\n",
            "53     54  23.479319\n",
            "54     55  23.418491\n",
            "55     56  23.479319\n",
            "56     57  20.133820\n",
            "57     58  23.965937\n",
            "58     59  26.520681\n",
            "59     60  27.189781\n",
            "60     61  22.080292\n",
            "61     62  22.019465\n",
            "62     63  26.216545\n",
            "63     64  26.155718\n",
            "64     65  21.897810\n",
            "65     66  26.155718\n",
            "66     67  26.399027\n",
            "67     68  27.189781\n",
            "68     69  27.250608\n",
            "69     70  26.946472\n",
            "70     71  27.128954\n",
            "71     72  18.309002\n",
            "72     73  10.036496\n",
            "73     74  17.700730\n",
            "74     75   8.394161\n",
            "75     76   8.394161\n",
            "76     77   8.394161\n",
            "77     78   8.394161\n",
            "78     79   8.394161\n",
            "79     80  14.841849\n",
            "80     81  15.571776\n",
            "81     82  15.571776\n",
            "82     83  23.479319\n",
            "83     84  19.586375\n",
            "84     85  15.571776\n",
            "85     86  15.571776\n",
            "86     87  13.807786\n",
            "87     88  14.841849\n",
            "88     89  13.199513\n",
            "89     90  14.841849\n",
            "90     91  14.841849\n",
            "91     92  12.712895\n",
            "92     93  18.795620\n",
            "93     94  18.126521\n",
            "94     95  15.997567\n",
            "95     96  17.092457\n",
            "96     97  15.267640\n",
            "97     98  18.126521\n",
            "98     99  18.309002\n",
            "99    100  17.274939\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!rm -r ./semi_supervised_learning_with_gan ;git clone https://github.com/iamardian/semi_supervised_learning_with_gan.git\n",
        "# !python ./semi_supervised_learning_with_gan/ssgan_parsbert.py -d persiannews -p 0.5\n",
        "!python ./semi_supervised_learning_with_gan/ecgan_parsbert.py -d persiannews -p 0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MxznXs1WoSc-"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}